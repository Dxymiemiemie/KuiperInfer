7767517
1332 1331
pnnx.Input               pnnx_input_0             0 1 x.3 #x.3=(1,1,500,128)f32
pnnx.Expression          pnnx_expr_1541           0 1 165 expr=2
pnnx.Expression          pnnx_expr_1540           0 1 166 expr=3
pnnx.Expression          pnnx_expr_1539           0 1 257 expr=None
pnnx.Attribute           effnet._conv_stem        0 1 weight.3 @data=(32,1,3,3)f32 #weight.3=(32,1,3,3)f32
torch.transpose          torch.transpose_227      3 1 x.3 165 166 input.3 $input=x.3 $dim0=165 $dim1=166 #x.3=(1,1,500,128)f32 #input.3=(1,1,128,500)f32
nn.ZeroPad2d             effnet._conv_stem.static_padding 1 1 input.3 263 padding=(0,1,0,1) #input.3=(1,1,128,500)f32 #263=(1,1,129,501)f32
pnnx.Expression          pnnx_expr_1534           0 1 264 expr=[2,2]
pnnx.Expression          pnnx_expr_1532           0 1 265 expr=[0,0]
pnnx.Expression          pnnx_expr_1530           0 1 266 expr=[1,1]
pnnx.Expression          pnnx_expr_1529           0 1 2377 expr=1
F.conv2d                 F.conv2d_23              7 1 263 weight.3 257 264 265 266 2377 input0.1 $input=263 $weight=weight.3 $bias=257 $stride=264 $padding=265 $dilation=266 $groups=2377 #263=(1,1,129,501)f32 #weight.3=(32,1,3,3)f32 #input0.1=(1,32,64,250)f32
nn.BatchNorm2d           effnet._bn0              1 1 input0.1 172 affine=True eps=1.000000e-03 num_features=32 @bias=(32)f32 @running_mean=(32)f32 @running_var=(32)f32 @weight=(32)f32 #input0.1=(1,32,64,250)f32 #172=(1,32,64,250)f32
nn.SiLU                  effnet._swish            1 1 172 176 #172=(1,32,64,250)f32 #176=(1,32,64,250)f32
pnnx.Expression          pnnx_expr_1527           0 1 279 expr=None
pnnx.Expression          pnnx_expr_1524           0 1 282 expr=32
pnnx.Attribute           effnet._blocks.0._depthwise_conv 0 1 weight.5 @data=(32,1,3,3)f32 #weight.5=(32,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.0._depthwise_conv.static_padding 1 1 176 285 padding=(1,1,1,1) #176=(1,32,64,250)f32 #285=(1,32,66,252)f32
pnnx.Expression          pnnx_expr_1522           0 1 286 expr=[1,1]
pnnx.Expression          pnnx_expr_1520           0 1 287 expr=[0,0]
pnnx.Expression          pnnx_expr_1517           0 1 288 expr=[1,1]
F.conv2d                 F.conv2d_24              7 1 285 weight.5 279 286 287 288 282 input.5 $input=285 $weight=weight.5 $bias=279 $stride=286 $padding=287 $dilation=288 $groups=282 #285=(1,32,66,252)f32 #weight.5=(32,1,3,3)f32 #input.5=(1,32,64,250)f32
nn.BatchNorm2d           effnet._blocks.0._bn1    1 1 input.5 291 affine=True eps=1.000000e-03 num_features=32 @bias=(32)f32 @running_mean=(32)f32 @running_var=(32)f32 @weight=(32)f32 #input.5=(1,32,64,250)f32 #291=(1,32,64,250)f32
nn.SiLU                  effnet._blocks.0._swish  1 1 291 292 #291=(1,32,64,250)f32 #292=(1,32,64,250)f32
pnnx.Expression          pnnx_expr_1515           0 1 293 expr=[1,1]
pnnx.Attribute           effnet._blocks.0._se_reduce 0 1 bias.3 @data=(8)f32 #bias.3=(8)f32
pnnx.Attribute           pnnx_unique_0            0 1 weight.7 @data=(8,32,1,1)f32 #weight.7=(8,32,1,1)f32
pnnx.Expression          pnnx_expr_1511           0 1 303 expr=[1,1]
pnnx.Expression          pnnx_expr_1509           0 1 304 expr=[0,0]
pnnx.Expression          pnnx_expr_1506           0 1 305 expr=[1,1]
pnnx.Expression          pnnx_expr_1505           0 1 2397 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_0  2 1 292 293 x.5 $input=292 $output_size=293 #292=(1,32,64,250)f32 #x.5=(1,32,1,1)f32
F.conv2d                 F.conv2d_25              7 1 x.5 weight.7 bias.3 303 304 305 2397 input.7 $input=x.5 $weight=weight.7 $bias=bias.3 $stride=303 $padding=304 $dilation=305 $groups=2397 #x.5=(1,32,1,1)f32 #weight.7=(8,32,1,1)f32 #bias.3=(8)f32 #input.7=(1,8,1,1)f32
nn.SiLU                  pnnx_unique_1            1 1 input.7 308 #input.7=(1,8,1,1)f32 #308=(1,8,1,1)f32
pnnx.Attribute           effnet._blocks.0._se_expand 0 1 bias.5 @data=(32)f32 #bias.5=(32)f32
pnnx.Attribute           pnnx_unique_2            0 1 weight.9 @data=(32,8,1,1)f32 #weight.9=(32,8,1,1)f32
pnnx.Expression          pnnx_expr_1501           0 1 317 expr=[1,1]
pnnx.Expression          pnnx_expr_1499           0 1 318 expr=[0,0]
pnnx.Expression          pnnx_expr_1496           0 1 319 expr=[1,1]
pnnx.Expression          pnnx_expr_1495           0 1 2407 expr=1
F.conv2d                 F.conv2d_26              7 1 308 weight.9 bias.5 317 318 319 2407 x_squeezed.2 $input=308 $weight=weight.9 $bias=bias.5 $stride=317 $padding=318 $dilation=319 $groups=2407 #308=(1,8,1,1)f32 #weight.9=(32,8,1,1)f32 #bias.5=(32)f32 #x_squeezed.2=(1,32,1,1)f32
F.sigmoid                F.sigmoid_138            1 1 x_squeezed.2 322 $input=x_squeezed.2 #x_squeezed.2=(1,32,1,1)f32 #322=(1,32,1,1)f32
pnnx.Expression          pnnx_expr_1494           2 1 322 292 x0.2 expr=mul(@0,@1) #322=(1,32,1,1)f32 #292=(1,32,64,250)f32 #x0.2=(1,32,64,250)f32
pnnx.Expression          pnnx_expr_1493           0 1 326 expr=None
pnnx.Attribute           effnet._blocks.0._project_conv 0 1 weight.11 @data=(16,32,1,1)f32 #weight.11=(16,32,1,1)f32
pnnx.Expression          pnnx_expr_1489           0 1 332 expr=[1,1]
pnnx.Expression          pnnx_expr_1487           0 1 333 expr=[0,0]
pnnx.Expression          pnnx_expr_1484           0 1 334 expr=[1,1]
pnnx.Expression          pnnx_expr_1483           0 1 2417 expr=1
F.conv2d                 F.conv2d_27              7 1 x0.2 weight.11 326 332 333 334 2417 input.9 $input=x0.2 $weight=weight.11 $bias=326 $stride=332 $padding=333 $dilation=334 $groups=2417 #x0.2=(1,32,64,250)f32 #weight.11=(16,32,1,1)f32 #input.9=(1,16,64,250)f32
nn.BatchNorm2d           effnet._blocks.0._bn2    1 1 input.9 337 affine=True eps=1.000000e-03 num_features=16 @bias=(16)f32 @running_mean=(16)f32 @running_var=(16)f32 @weight=(16)f32 #input.9=(1,16,64,250)f32 #337=(1,16,64,250)f32
pnnx.Expression          pnnx_expr_1481           0 1 348 expr=None
pnnx.Expression          pnnx_expr_1478           0 1 351 expr=16
pnnx.Attribute           effnet._blocks.1._depthwise_conv 0 1 weight.13 @data=(16,1,3,3)f32 #weight.13=(16,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.1._depthwise_conv.static_padding 1 1 337 354 padding=(1,1,1,1) #337=(1,16,64,250)f32 #354=(1,16,66,252)f32
pnnx.Expression          pnnx_expr_1476           0 1 355 expr=[1,1]
pnnx.Expression          pnnx_expr_1474           0 1 356 expr=[0,0]
pnnx.Expression          pnnx_expr_1471           0 1 357 expr=[1,1]
F.conv2d                 F.conv2d_28              7 1 354 weight.13 348 355 356 357 351 input.11 $input=354 $weight=weight.13 $bias=348 $stride=355 $padding=356 $dilation=357 $groups=351 #354=(1,16,66,252)f32 #weight.13=(16,1,3,3)f32 #input.11=(1,16,64,250)f32
nn.BatchNorm2d           effnet._blocks.1._bn1    1 1 input.11 360 affine=True eps=1.000000e-03 num_features=16 @bias=(16)f32 @running_mean=(16)f32 @running_var=(16)f32 @weight=(16)f32 #input.11=(1,16,64,250)f32 #360=(1,16,64,250)f32
nn.SiLU                  effnet._blocks.1._swish  1 1 360 361 #360=(1,16,64,250)f32 #361=(1,16,64,250)f32
pnnx.Expression          pnnx_expr_1469           0 1 362 expr=[1,1]
pnnx.Attribute           effnet._blocks.1._se_reduce 0 1 bias.7 @data=(4)f32 #bias.7=(4)f32
pnnx.Attribute           pnnx_unique_3            0 1 weight.15 @data=(4,16,1,1)f32 #weight.15=(4,16,1,1)f32
pnnx.Expression          pnnx_expr_1465           0 1 372 expr=[1,1]
pnnx.Expression          pnnx_expr_1463           0 1 373 expr=[0,0]
pnnx.Expression          pnnx_expr_1460           0 1 374 expr=[1,1]
pnnx.Expression          pnnx_expr_1459           0 1 2437 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_1  2 1 361 362 x.7 $input=361 $output_size=362 #361=(1,16,64,250)f32 #x.7=(1,16,1,1)f32
F.conv2d                 F.conv2d_29              7 1 x.7 weight.15 bias.7 372 373 374 2437 input.13 $input=x.7 $weight=weight.15 $bias=bias.7 $stride=372 $padding=373 $dilation=374 $groups=2437 #x.7=(1,16,1,1)f32 #weight.15=(4,16,1,1)f32 #bias.7=(4)f32 #input.13=(1,4,1,1)f32
nn.SiLU                  pnnx_unique_4            1 1 input.13 377 #input.13=(1,4,1,1)f32 #377=(1,4,1,1)f32
pnnx.Attribute           effnet._blocks.1._se_expand 0 1 bias.9 @data=(16)f32 #bias.9=(16)f32
pnnx.Attribute           pnnx_unique_5            0 1 weight.17 @data=(16,4,1,1)f32 #weight.17=(16,4,1,1)f32
pnnx.Expression          pnnx_expr_1455           0 1 386 expr=[1,1]
pnnx.Expression          pnnx_expr_1453           0 1 387 expr=[0,0]
pnnx.Expression          pnnx_expr_1450           0 1 388 expr=[1,1]
pnnx.Expression          pnnx_expr_1449           0 1 2447 expr=1
F.conv2d                 F.conv2d_30              7 1 377 weight.17 bias.9 386 387 388 2447 x_squeezed.4 $input=377 $weight=weight.17 $bias=bias.9 $stride=386 $padding=387 $dilation=388 $groups=2447 #377=(1,4,1,1)f32 #weight.17=(16,4,1,1)f32 #bias.9=(16)f32 #x_squeezed.4=(1,16,1,1)f32
F.sigmoid                F.sigmoid_139            1 1 x_squeezed.4 391 $input=x_squeezed.4 #x_squeezed.4=(1,16,1,1)f32 #391=(1,16,1,1)f32
pnnx.Expression          pnnx_expr_1448           2 1 391 361 x0.4 expr=mul(@0,@1) #391=(1,16,1,1)f32 #361=(1,16,64,250)f32 #x0.4=(1,16,64,250)f32
pnnx.Expression          pnnx_expr_1447           0 1 395 expr=None
pnnx.Attribute           effnet._blocks.1._project_conv 0 1 weight.19 @data=(16,16,1,1)f32 #weight.19=(16,16,1,1)f32
pnnx.Expression          pnnx_expr_1443           0 1 401 expr=[1,1]
pnnx.Expression          pnnx_expr_1441           0 1 402 expr=[0,0]
pnnx.Expression          pnnx_expr_1438           0 1 403 expr=[1,1]
pnnx.Expression          pnnx_expr_1437           0 1 2457 expr=1
F.conv2d                 F.conv2d_31              7 1 x0.4 weight.19 395 401 402 403 2457 input.15 $input=x0.4 $weight=weight.19 $bias=395 $stride=401 $padding=402 $dilation=403 $groups=2457 #x0.4=(1,16,64,250)f32 #weight.19=(16,16,1,1)f32 #input.15=(1,16,64,250)f32
nn.BatchNorm2d           effnet._blocks.1._bn2    1 1 input.15 406 affine=True eps=1.000000e-03 num_features=16 @bias=(16)f32 @running_mean=(16)f32 @running_var=(16)f32 @weight=(16)f32 #input.15=(1,16,64,250)f32 #406=(1,16,64,250)f32
pnnx.Expression          pnnx_expr_1435           2 1 406 337 407 expr=add(@0,@1) #406=(1,16,64,250)f32 #337=(1,16,64,250)f32 #407=(1,16,64,250)f32
pnnx.Expression          pnnx_expr_1433           0 1 420 expr=None
pnnx.Attribute           effnet._blocks.2._expand_conv 0 1 weight.21 @data=(96,16,1,1)f32 #weight.21=(96,16,1,1)f32
pnnx.Expression          pnnx_expr_1429           0 1 426 expr=[1,1]
pnnx.Expression          pnnx_expr_1427           0 1 427 expr=[0,0]
pnnx.Expression          pnnx_expr_1424           0 1 428 expr=[1,1]
pnnx.Expression          pnnx_expr_1423           0 1 2468 expr=1
F.conv2d                 F.conv2d_32              7 1 407 weight.21 420 426 427 428 2468 input.17 $input=407 $weight=weight.21 $bias=420 $stride=426 $padding=427 $dilation=428 $groups=2468 #407=(1,16,64,250)f32 #weight.21=(96,16,1,1)f32 #input.17=(1,96,64,250)f32
nn.BatchNorm2d           effnet._blocks.2._bn0    1 1 input.17 431 affine=True eps=1.000000e-03 num_features=96 @bias=(96)f32 @running_mean=(96)f32 @running_var=(96)f32 @weight=(96)f32 #input.17=(1,96,64,250)f32 #431=(1,96,64,250)f32
nn.SiLU                  effnet._blocks.2._swish  1 1 431 432 #431=(1,96,64,250)f32 #432=(1,96,64,250)f32
pnnx.Expression          pnnx_expr_1422           0 1 435 expr=None
pnnx.Expression          pnnx_expr_1418           0 1 439 expr=96
pnnx.Attribute           effnet._blocks.2._depthwise_conv 0 1 weight.23 @data=(96,1,3,3)f32 #weight.23=(96,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.2._depthwise_conv.static_padding 1 1 432 442 padding=(0,1,0,1) #432=(1,96,64,250)f32 #442=(1,96,65,251)f32
pnnx.Expression          pnnx_expr_1416           0 1 443 expr=[2,2]
pnnx.Expression          pnnx_expr_1414           0 1 444 expr=[0,0]
pnnx.Expression          pnnx_expr_1412           0 1 445 expr=[1,1]
F.conv2d                 F.conv2d_33              7 1 442 weight.23 435 443 444 445 439 input.19 $input=442 $weight=weight.23 $bias=435 $stride=443 $padding=444 $dilation=445 $groups=439 #442=(1,96,65,251)f32 #weight.23=(96,1,3,3)f32 #input.19=(1,96,32,125)f32
nn.BatchNorm2d           effnet._blocks.2._bn1    1 1 input.19 448 affine=True eps=1.000000e-03 num_features=96 @bias=(96)f32 @running_mean=(96)f32 @running_var=(96)f32 @weight=(96)f32 #input.19=(1,96,32,125)f32 #448=(1,96,32,125)f32
nn.SiLU                  pnnx_unique_6            1 1 448 449 #448=(1,96,32,125)f32 #449=(1,96,32,125)f32
pnnx.Expression          pnnx_expr_1410           0 1 450 expr=[1,1]
pnnx.Attribute           effnet._blocks.2._se_reduce 0 1 bias.11 @data=(4)f32 #bias.11=(4)f32
pnnx.Attribute           pnnx_unique_7            0 1 weight.25 @data=(4,96,1,1)f32 #weight.25=(4,96,1,1)f32
pnnx.Expression          pnnx_expr_1406           0 1 460 expr=[1,1]
pnnx.Expression          pnnx_expr_1404           0 1 461 expr=[0,0]
pnnx.Expression          pnnx_expr_1401           0 1 462 expr=[1,1]
pnnx.Expression          pnnx_expr_1400           0 1 2487 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_2  2 1 449 450 x.9 $input=449 $output_size=450 #449=(1,96,32,125)f32 #x.9=(1,96,1,1)f32
F.conv2d                 F.conv2d_34              7 1 x.9 weight.25 bias.11 460 461 462 2487 input.21 $input=x.9 $weight=weight.25 $bias=bias.11 $stride=460 $padding=461 $dilation=462 $groups=2487 #x.9=(1,96,1,1)f32 #weight.25=(4,96,1,1)f32 #bias.11=(4)f32 #input.21=(1,4,1,1)f32
nn.SiLU                  pnnx_unique_8            1 1 input.21 465 #input.21=(1,4,1,1)f32 #465=(1,4,1,1)f32
pnnx.Attribute           effnet._blocks.2._se_expand 0 1 bias.13 @data=(96)f32 #bias.13=(96)f32
pnnx.Attribute           pnnx_unique_9            0 1 weight.27 @data=(96,4,1,1)f32 #weight.27=(96,4,1,1)f32
pnnx.Expression          pnnx_expr_1396           0 1 474 expr=[1,1]
pnnx.Expression          pnnx_expr_1394           0 1 475 expr=[0,0]
pnnx.Expression          pnnx_expr_1391           0 1 476 expr=[1,1]
pnnx.Expression          pnnx_expr_1390           0 1 2497 expr=1
F.conv2d                 F.conv2d_35              7 1 465 weight.27 bias.13 474 475 476 2497 x_squeezed.6 $input=465 $weight=weight.27 $bias=bias.13 $stride=474 $padding=475 $dilation=476 $groups=2497 #465=(1,4,1,1)f32 #weight.27=(96,4,1,1)f32 #bias.13=(96)f32 #x_squeezed.6=(1,96,1,1)f32
F.sigmoid                F.sigmoid_140            1 1 x_squeezed.6 479 $input=x_squeezed.6 #x_squeezed.6=(1,96,1,1)f32 #479=(1,96,1,1)f32
pnnx.Expression          pnnx_expr_1389           2 1 479 449 x0.6 expr=mul(@0,@1) #479=(1,96,1,1)f32 #449=(1,96,32,125)f32 #x0.6=(1,96,32,125)f32
pnnx.Expression          pnnx_expr_1388           0 1 483 expr=None
pnnx.Attribute           effnet._blocks.2._project_conv 0 1 weight.29 @data=(24,96,1,1)f32 #weight.29=(24,96,1,1)f32
pnnx.Expression          pnnx_expr_1384           0 1 489 expr=[1,1]
pnnx.Expression          pnnx_expr_1382           0 1 490 expr=[0,0]
pnnx.Expression          pnnx_expr_1379           0 1 491 expr=[1,1]
pnnx.Expression          pnnx_expr_1378           0 1 2507 expr=1
F.conv2d                 F.conv2d_36              7 1 x0.6 weight.29 483 489 490 491 2507 input.23 $input=x0.6 $weight=weight.29 $bias=483 $stride=489 $padding=490 $dilation=491 $groups=2507 #x0.6=(1,96,32,125)f32 #weight.29=(24,96,1,1)f32 #input.23=(1,24,32,125)f32
nn.BatchNorm2d           effnet._blocks.2._bn2    1 1 input.23 494 affine=True eps=1.000000e-03 num_features=24 @bias=(24)f32 @running_mean=(24)f32 @running_var=(24)f32 @weight=(24)f32 #input.23=(1,24,32,125)f32 #494=(1,24,32,125)f32
pnnx.Expression          pnnx_expr_1376           0 1 507 expr=None
pnnx.Attribute           effnet._blocks.3._expand_conv 0 1 weight.31 @data=(144,24,1,1)f32 #weight.31=(144,24,1,1)f32
pnnx.Expression          pnnx_expr_1372           0 1 513 expr=[1,1]
pnnx.Expression          pnnx_expr_1370           0 1 514 expr=[0,0]
pnnx.Expression          pnnx_expr_1367           0 1 515 expr=[1,1]
pnnx.Expression          pnnx_expr_1366           0 1 2517 expr=1
F.conv2d                 F.conv2d_37              7 1 494 weight.31 507 513 514 515 2517 input.25 $input=494 $weight=weight.31 $bias=507 $stride=513 $padding=514 $dilation=515 $groups=2517 #494=(1,24,32,125)f32 #weight.31=(144,24,1,1)f32 #input.25=(1,144,32,125)f32
nn.BatchNorm2d           effnet._blocks.3._bn0    1 1 input.25 518 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.25=(1,144,32,125)f32 #518=(1,144,32,125)f32
nn.SiLU                  effnet._blocks.3._swish  1 1 518 519 #518=(1,144,32,125)f32 #519=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1365           0 1 522 expr=None
pnnx.Expression          pnnx_expr_1362           0 1 525 expr=144
pnnx.Attribute           effnet._blocks.3._depthwise_conv 0 1 weight.33 @data=(144,1,3,3)f32 #weight.33=(144,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.3._depthwise_conv.static_padding 1 1 519 528 padding=(1,1,1,1) #519=(1,144,32,125)f32 #528=(1,144,34,127)f32
pnnx.Expression          pnnx_expr_1360           0 1 529 expr=[1,1]
pnnx.Expression          pnnx_expr_1358           0 1 530 expr=[0,0]
pnnx.Expression          pnnx_expr_1355           0 1 531 expr=[1,1]
F.conv2d                 F.conv2d_38              7 1 528 weight.33 522 529 530 531 525 input.27 $input=528 $weight=weight.33 $bias=522 $stride=529 $padding=530 $dilation=531 $groups=525 #528=(1,144,34,127)f32 #weight.33=(144,1,3,3)f32 #input.27=(1,144,32,125)f32
nn.BatchNorm2d           effnet._blocks.3._bn1    1 1 input.27 534 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.27=(1,144,32,125)f32 #534=(1,144,32,125)f32
nn.SiLU                  pnnx_unique_10           1 1 534 535 #534=(1,144,32,125)f32 #535=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1353           0 1 536 expr=[1,1]
pnnx.Attribute           effnet._blocks.3._se_reduce 0 1 bias.15 @data=(6)f32 #bias.15=(6)f32
pnnx.Attribute           pnnx_unique_11           0 1 weight.35 @data=(6,144,1,1)f32 #weight.35=(6,144,1,1)f32
pnnx.Expression          pnnx_expr_1349           0 1 546 expr=[1,1]
pnnx.Expression          pnnx_expr_1347           0 1 547 expr=[0,0]
pnnx.Expression          pnnx_expr_1344           0 1 548 expr=[1,1]
pnnx.Expression          pnnx_expr_1343           0 1 2537 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_3  2 1 535 536 x.11 $input=535 $output_size=536 #535=(1,144,32,125)f32 #x.11=(1,144,1,1)f32
F.conv2d                 F.conv2d_39              7 1 x.11 weight.35 bias.15 546 547 548 2537 input.29 $input=x.11 $weight=weight.35 $bias=bias.15 $stride=546 $padding=547 $dilation=548 $groups=2537 #x.11=(1,144,1,1)f32 #weight.35=(6,144,1,1)f32 #bias.15=(6)f32 #input.29=(1,6,1,1)f32
nn.SiLU                  pnnx_unique_12           1 1 input.29 551 #input.29=(1,6,1,1)f32 #551=(1,6,1,1)f32
pnnx.Attribute           effnet._blocks.3._se_expand 0 1 bias.17 @data=(144)f32 #bias.17=(144)f32
pnnx.Attribute           pnnx_unique_13           0 1 weight.37 @data=(144,6,1,1)f32 #weight.37=(144,6,1,1)f32
pnnx.Expression          pnnx_expr_1339           0 1 560 expr=[1,1]
pnnx.Expression          pnnx_expr_1337           0 1 561 expr=[0,0]
pnnx.Expression          pnnx_expr_1334           0 1 562 expr=[1,1]
pnnx.Expression          pnnx_expr_1333           0 1 2547 expr=1
F.conv2d                 F.conv2d_40              7 1 551 weight.37 bias.17 560 561 562 2547 x_squeezed.8 $input=551 $weight=weight.37 $bias=bias.17 $stride=560 $padding=561 $dilation=562 $groups=2547 #551=(1,6,1,1)f32 #weight.37=(144,6,1,1)f32 #bias.17=(144)f32 #x_squeezed.8=(1,144,1,1)f32
F.sigmoid                F.sigmoid_141            1 1 x_squeezed.8 565 $input=x_squeezed.8 #x_squeezed.8=(1,144,1,1)f32 #565=(1,144,1,1)f32
pnnx.Expression          pnnx_expr_1332           2 1 565 535 x0.8 expr=mul(@0,@1) #565=(1,144,1,1)f32 #535=(1,144,32,125)f32 #x0.8=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1331           0 1 569 expr=None
pnnx.Attribute           effnet._blocks.3._project_conv 0 1 weight.39 @data=(24,144,1,1)f32 #weight.39=(24,144,1,1)f32
pnnx.Expression          pnnx_expr_1327           0 1 575 expr=[1,1]
pnnx.Expression          pnnx_expr_1325           0 1 576 expr=[0,0]
pnnx.Expression          pnnx_expr_1322           0 1 577 expr=[1,1]
pnnx.Expression          pnnx_expr_1321           0 1 2557 expr=1
F.conv2d                 F.conv2d_41              7 1 x0.8 weight.39 569 575 576 577 2557 input.31 $input=x0.8 $weight=weight.39 $bias=569 $stride=575 $padding=576 $dilation=577 $groups=2557 #x0.8=(1,144,32,125)f32 #weight.39=(24,144,1,1)f32 #input.31=(1,24,32,125)f32
nn.BatchNorm2d           effnet._blocks.3._bn2    1 1 input.31 580 affine=True eps=1.000000e-03 num_features=24 @bias=(24)f32 @running_mean=(24)f32 @running_var=(24)f32 @weight=(24)f32 #input.31=(1,24,32,125)f32 #580=(1,24,32,125)f32
pnnx.Expression          pnnx_expr_1319           2 1 580 494 581 expr=add(@0,@1) #580=(1,24,32,125)f32 #494=(1,24,32,125)f32 #581=(1,24,32,125)f32
pnnx.Expression          pnnx_expr_1317           0 1 594 expr=None
pnnx.Attribute           effnet._blocks.4._expand_conv 0 1 weight.41 @data=(144,24,1,1)f32 #weight.41=(144,24,1,1)f32
pnnx.Expression          pnnx_expr_1313           0 1 600 expr=[1,1]
pnnx.Expression          pnnx_expr_1311           0 1 601 expr=[0,0]
pnnx.Expression          pnnx_expr_1308           0 1 602 expr=[1,1]
pnnx.Expression          pnnx_expr_1307           0 1 2568 expr=1
F.conv2d                 F.conv2d_42              7 1 581 weight.41 594 600 601 602 2568 input.33 $input=581 $weight=weight.41 $bias=594 $stride=600 $padding=601 $dilation=602 $groups=2568 #581=(1,24,32,125)f32 #weight.41=(144,24,1,1)f32 #input.33=(1,144,32,125)f32
nn.BatchNorm2d           effnet._blocks.4._bn0    1 1 input.33 605 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.33=(1,144,32,125)f32 #605=(1,144,32,125)f32
nn.SiLU                  effnet._blocks.4._swish  1 1 605 606 #605=(1,144,32,125)f32 #606=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1306           0 1 609 expr=None
pnnx.Expression          pnnx_expr_1303           0 1 612 expr=144
pnnx.Attribute           effnet._blocks.4._depthwise_conv 0 1 weight.43 @data=(144,1,3,3)f32 #weight.43=(144,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.4._depthwise_conv.static_padding 1 1 606 615 padding=(1,1,1,1) #606=(1,144,32,125)f32 #615=(1,144,34,127)f32
pnnx.Expression          pnnx_expr_1301           0 1 616 expr=[1,1]
pnnx.Expression          pnnx_expr_1299           0 1 617 expr=[0,0]
pnnx.Expression          pnnx_expr_1296           0 1 618 expr=[1,1]
F.conv2d                 F.conv2d_43              7 1 615 weight.43 609 616 617 618 612 input.35 $input=615 $weight=weight.43 $bias=609 $stride=616 $padding=617 $dilation=618 $groups=612 #615=(1,144,34,127)f32 #weight.43=(144,1,3,3)f32 #input.35=(1,144,32,125)f32
nn.BatchNorm2d           effnet._blocks.4._bn1    1 1 input.35 621 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.35=(1,144,32,125)f32 #621=(1,144,32,125)f32
nn.SiLU                  pnnx_unique_14           1 1 621 622 #621=(1,144,32,125)f32 #622=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1294           0 1 623 expr=[1,1]
pnnx.Attribute           effnet._blocks.4._se_reduce 0 1 bias.19 @data=(6)f32 #bias.19=(6)f32
pnnx.Attribute           pnnx_unique_15           0 1 weight.45 @data=(6,144,1,1)f32 #weight.45=(6,144,1,1)f32
pnnx.Expression          pnnx_expr_1290           0 1 633 expr=[1,1]
pnnx.Expression          pnnx_expr_1288           0 1 634 expr=[0,0]
pnnx.Expression          pnnx_expr_1285           0 1 635 expr=[1,1]
pnnx.Expression          pnnx_expr_1284           0 1 2588 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_4  2 1 622 623 x.13 $input=622 $output_size=623 #622=(1,144,32,125)f32 #x.13=(1,144,1,1)f32
F.conv2d                 F.conv2d_44              7 1 x.13 weight.45 bias.19 633 634 635 2588 input.37 $input=x.13 $weight=weight.45 $bias=bias.19 $stride=633 $padding=634 $dilation=635 $groups=2588 #x.13=(1,144,1,1)f32 #weight.45=(6,144,1,1)f32 #bias.19=(6)f32 #input.37=(1,6,1,1)f32
nn.SiLU                  pnnx_unique_16           1 1 input.37 638 #input.37=(1,6,1,1)f32 #638=(1,6,1,1)f32
pnnx.Attribute           effnet._blocks.4._se_expand 0 1 bias.21 @data=(144)f32 #bias.21=(144)f32
pnnx.Attribute           pnnx_unique_17           0 1 weight.47 @data=(144,6,1,1)f32 #weight.47=(144,6,1,1)f32
pnnx.Expression          pnnx_expr_1280           0 1 647 expr=[1,1]
pnnx.Expression          pnnx_expr_1278           0 1 648 expr=[0,0]
pnnx.Expression          pnnx_expr_1275           0 1 649 expr=[1,1]
pnnx.Expression          pnnx_expr_1274           0 1 2598 expr=1
F.conv2d                 F.conv2d_45              7 1 638 weight.47 bias.21 647 648 649 2598 x_squeezed.10 $input=638 $weight=weight.47 $bias=bias.21 $stride=647 $padding=648 $dilation=649 $groups=2598 #638=(1,6,1,1)f32 #weight.47=(144,6,1,1)f32 #bias.21=(144)f32 #x_squeezed.10=(1,144,1,1)f32
F.sigmoid                F.sigmoid_142            1 1 x_squeezed.10 652 $input=x_squeezed.10 #x_squeezed.10=(1,144,1,1)f32 #652=(1,144,1,1)f32
pnnx.Expression          pnnx_expr_1273           2 1 652 622 x0.10 expr=mul(@0,@1) #652=(1,144,1,1)f32 #622=(1,144,32,125)f32 #x0.10=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1272           0 1 656 expr=None
pnnx.Attribute           effnet._blocks.4._project_conv 0 1 weight.49 @data=(24,144,1,1)f32 #weight.49=(24,144,1,1)f32
pnnx.Expression          pnnx_expr_1268           0 1 662 expr=[1,1]
pnnx.Expression          pnnx_expr_1266           0 1 663 expr=[0,0]
pnnx.Expression          pnnx_expr_1263           0 1 664 expr=[1,1]
pnnx.Expression          pnnx_expr_1262           0 1 2608 expr=1
F.conv2d                 F.conv2d_46              7 1 x0.10 weight.49 656 662 663 664 2608 input.39 $input=x0.10 $weight=weight.49 $bias=656 $stride=662 $padding=663 $dilation=664 $groups=2608 #x0.10=(1,144,32,125)f32 #weight.49=(24,144,1,1)f32 #input.39=(1,24,32,125)f32
nn.BatchNorm2d           effnet._blocks.4._bn2    1 1 input.39 667 affine=True eps=1.000000e-03 num_features=24 @bias=(24)f32 @running_mean=(24)f32 @running_var=(24)f32 @weight=(24)f32 #input.39=(1,24,32,125)f32 #667=(1,24,32,125)f32
pnnx.Expression          pnnx_expr_1260           2 1 667 581 668 expr=add(@0,@1) #667=(1,24,32,125)f32 #581=(1,24,32,125)f32 #668=(1,24,32,125)f32
pnnx.Expression          pnnx_expr_1258           0 1 681 expr=None
pnnx.Attribute           effnet._blocks.5._expand_conv 0 1 weight.51 @data=(144,24,1,1)f32 #weight.51=(144,24,1,1)f32
pnnx.Expression          pnnx_expr_1254           0 1 687 expr=[1,1]
pnnx.Expression          pnnx_expr_1252           0 1 688 expr=[0,0]
pnnx.Expression          pnnx_expr_1249           0 1 689 expr=[1,1]
pnnx.Expression          pnnx_expr_1248           0 1 2619 expr=1
F.conv2d                 F.conv2d_47              7 1 668 weight.51 681 687 688 689 2619 input.41 $input=668 $weight=weight.51 $bias=681 $stride=687 $padding=688 $dilation=689 $groups=2619 #668=(1,24,32,125)f32 #weight.51=(144,24,1,1)f32 #input.41=(1,144,32,125)f32
nn.BatchNorm2d           effnet._blocks.5._bn0    1 1 input.41 692 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.41=(1,144,32,125)f32 #692=(1,144,32,125)f32
nn.SiLU                  effnet._blocks.5._swish  1 1 692 693 #692=(1,144,32,125)f32 #693=(1,144,32,125)f32
pnnx.Expression          pnnx_expr_1247           0 1 696 expr=None
pnnx.Expression          pnnx_expr_1243           0 1 700 expr=144
pnnx.Attribute           effnet._blocks.5._depthwise_conv 0 1 weight.53 @data=(144,1,5,5)f32 #weight.53=(144,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.5._depthwise_conv.static_padding 1 1 693 703 padding=(2,2,2,2) #693=(1,144,32,125)f32 #703=(1,144,36,129)f32
pnnx.Expression          pnnx_expr_1241           0 1 704 expr=[2,2]
pnnx.Expression          pnnx_expr_1239           0 1 705 expr=[0,0]
pnnx.Expression          pnnx_expr_1237           0 1 706 expr=[1,1]
F.conv2d                 F.conv2d_48              7 1 703 weight.53 696 704 705 706 700 input.43 $input=703 $weight=weight.53 $bias=696 $stride=704 $padding=705 $dilation=706 $groups=700 #703=(1,144,36,129)f32 #weight.53=(144,1,5,5)f32 #input.43=(1,144,16,63)f32
nn.BatchNorm2d           effnet._blocks.5._bn1    1 1 input.43 709 affine=True eps=1.000000e-03 num_features=144 @bias=(144)f32 @running_mean=(144)f32 @running_var=(144)f32 @weight=(144)f32 #input.43=(1,144,16,63)f32 #709=(1,144,16,63)f32
nn.SiLU                  pnnx_unique_18           1 1 709 710 #709=(1,144,16,63)f32 #710=(1,144,16,63)f32
pnnx.Expression          pnnx_expr_1235           0 1 711 expr=[1,1]
pnnx.Attribute           effnet._blocks.5._se_reduce 0 1 bias.23 @data=(6)f32 #bias.23=(6)f32
pnnx.Attribute           pnnx_unique_19           0 1 weight.55 @data=(6,144,1,1)f32 #weight.55=(6,144,1,1)f32
pnnx.Expression          pnnx_expr_1231           0 1 721 expr=[1,1]
pnnx.Expression          pnnx_expr_1229           0 1 722 expr=[0,0]
pnnx.Expression          pnnx_expr_1226           0 1 723 expr=[1,1]
pnnx.Expression          pnnx_expr_1225           0 1 2638 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_5  2 1 710 711 x.15 $input=710 $output_size=711 #710=(1,144,16,63)f32 #x.15=(1,144,1,1)f32
F.conv2d                 F.conv2d_49              7 1 x.15 weight.55 bias.23 721 722 723 2638 input.45 $input=x.15 $weight=weight.55 $bias=bias.23 $stride=721 $padding=722 $dilation=723 $groups=2638 #x.15=(1,144,1,1)f32 #weight.55=(6,144,1,1)f32 #bias.23=(6)f32 #input.45=(1,6,1,1)f32
nn.SiLU                  pnnx_unique_20           1 1 input.45 726 #input.45=(1,6,1,1)f32 #726=(1,6,1,1)f32
pnnx.Attribute           effnet._blocks.5._se_expand 0 1 bias.25 @data=(144)f32 #bias.25=(144)f32
pnnx.Attribute           pnnx_unique_21           0 1 weight.57 @data=(144,6,1,1)f32 #weight.57=(144,6,1,1)f32
pnnx.Expression          pnnx_expr_1221           0 1 735 expr=[1,1]
pnnx.Expression          pnnx_expr_1219           0 1 736 expr=[0,0]
pnnx.Expression          pnnx_expr_1216           0 1 737 expr=[1,1]
pnnx.Expression          pnnx_expr_1215           0 1 2648 expr=1
F.conv2d                 F.conv2d_50              7 1 726 weight.57 bias.25 735 736 737 2648 x_squeezed.12 $input=726 $weight=weight.57 $bias=bias.25 $stride=735 $padding=736 $dilation=737 $groups=2648 #726=(1,6,1,1)f32 #weight.57=(144,6,1,1)f32 #bias.25=(144)f32 #x_squeezed.12=(1,144,1,1)f32
F.sigmoid                F.sigmoid_143            1 1 x_squeezed.12 740 $input=x_squeezed.12 #x_squeezed.12=(1,144,1,1)f32 #740=(1,144,1,1)f32
pnnx.Expression          pnnx_expr_1214           2 1 740 710 x0.12 expr=mul(@0,@1) #740=(1,144,1,1)f32 #710=(1,144,16,63)f32 #x0.12=(1,144,16,63)f32
pnnx.Expression          pnnx_expr_1213           0 1 744 expr=None
pnnx.Attribute           effnet._blocks.5._project_conv 0 1 weight.59 @data=(48,144,1,1)f32 #weight.59=(48,144,1,1)f32
pnnx.Expression          pnnx_expr_1209           0 1 750 expr=[1,1]
pnnx.Expression          pnnx_expr_1207           0 1 751 expr=[0,0]
pnnx.Expression          pnnx_expr_1204           0 1 752 expr=[1,1]
pnnx.Expression          pnnx_expr_1203           0 1 2658 expr=1
F.conv2d                 F.conv2d_51              7 1 x0.12 weight.59 744 750 751 752 2658 input.47 $input=x0.12 $weight=weight.59 $bias=744 $stride=750 $padding=751 $dilation=752 $groups=2658 #x0.12=(1,144,16,63)f32 #weight.59=(48,144,1,1)f32 #input.47=(1,48,16,63)f32
nn.BatchNorm2d           effnet._blocks.5._bn2    1 1 input.47 755 affine=True eps=1.000000e-03 num_features=48 @bias=(48)f32 @running_mean=(48)f32 @running_var=(48)f32 @weight=(48)f32 #input.47=(1,48,16,63)f32 #755=(1,48,16,63)f32
pnnx.Expression          pnnx_expr_1201           0 1 768 expr=None
pnnx.Attribute           effnet._blocks.6._expand_conv 0 1 weight.61 @data=(288,48,1,1)f32 #weight.61=(288,48,1,1)f32
pnnx.Expression          pnnx_expr_1197           0 1 774 expr=[1,1]
pnnx.Expression          pnnx_expr_1195           0 1 775 expr=[0,0]
pnnx.Expression          pnnx_expr_1192           0 1 776 expr=[1,1]
pnnx.Expression          pnnx_expr_1191           0 1 2668 expr=1
F.conv2d                 F.conv2d_52              7 1 755 weight.61 768 774 775 776 2668 input.49 $input=755 $weight=weight.61 $bias=768 $stride=774 $padding=775 $dilation=776 $groups=2668 #755=(1,48,16,63)f32 #weight.61=(288,48,1,1)f32 #input.49=(1,288,16,63)f32
nn.BatchNorm2d           effnet._blocks.6._bn0    1 1 input.49 779 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.49=(1,288,16,63)f32 #779=(1,288,16,63)f32
nn.SiLU                  effnet._blocks.6._swish  1 1 779 780 #779=(1,288,16,63)f32 #780=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1190           0 1 783 expr=None
pnnx.Expression          pnnx_expr_1187           0 1 786 expr=288
pnnx.Attribute           effnet._blocks.6._depthwise_conv 0 1 weight.63 @data=(288,1,5,5)f32 #weight.63=(288,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.6._depthwise_conv.static_padding 1 1 780 789 padding=(2,2,2,2) #780=(1,288,16,63)f32 #789=(1,288,20,67)f32
pnnx.Expression          pnnx_expr_1185           0 1 790 expr=[1,1]
pnnx.Expression          pnnx_expr_1183           0 1 791 expr=[0,0]
pnnx.Expression          pnnx_expr_1180           0 1 792 expr=[1,1]
F.conv2d                 F.conv2d_53              7 1 789 weight.63 783 790 791 792 786 input.51 $input=789 $weight=weight.63 $bias=783 $stride=790 $padding=791 $dilation=792 $groups=786 #789=(1,288,20,67)f32 #weight.63=(288,1,5,5)f32 #input.51=(1,288,16,63)f32
nn.BatchNorm2d           effnet._blocks.6._bn1    1 1 input.51 795 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.51=(1,288,16,63)f32 #795=(1,288,16,63)f32
nn.SiLU                  pnnx_unique_22           1 1 795 796 #795=(1,288,16,63)f32 #796=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1178           0 1 797 expr=[1,1]
pnnx.Attribute           effnet._blocks.6._se_reduce 0 1 bias.27 @data=(12)f32 #bias.27=(12)f32
pnnx.Attribute           pnnx_unique_23           0 1 weight.65 @data=(12,288,1,1)f32 #weight.65=(12,288,1,1)f32
pnnx.Expression          pnnx_expr_1174           0 1 807 expr=[1,1]
pnnx.Expression          pnnx_expr_1172           0 1 808 expr=[0,0]
pnnx.Expression          pnnx_expr_1169           0 1 809 expr=[1,1]
pnnx.Expression          pnnx_expr_1168           0 1 2688 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_6  2 1 796 797 x.17 $input=796 $output_size=797 #796=(1,288,16,63)f32 #x.17=(1,288,1,1)f32
F.conv2d                 F.conv2d_54              7 1 x.17 weight.65 bias.27 807 808 809 2688 input.53 $input=x.17 $weight=weight.65 $bias=bias.27 $stride=807 $padding=808 $dilation=809 $groups=2688 #x.17=(1,288,1,1)f32 #weight.65=(12,288,1,1)f32 #bias.27=(12)f32 #input.53=(1,12,1,1)f32
nn.SiLU                  pnnx_unique_24           1 1 input.53 812 #input.53=(1,12,1,1)f32 #812=(1,12,1,1)f32
pnnx.Attribute           effnet._blocks.6._se_expand 0 1 bias.29 @data=(288)f32 #bias.29=(288)f32
pnnx.Attribute           pnnx_unique_25           0 1 weight.67 @data=(288,12,1,1)f32 #weight.67=(288,12,1,1)f32
pnnx.Expression          pnnx_expr_1164           0 1 821 expr=[1,1]
pnnx.Expression          pnnx_expr_1162           0 1 822 expr=[0,0]
pnnx.Expression          pnnx_expr_1159           0 1 823 expr=[1,1]
pnnx.Expression          pnnx_expr_1158           0 1 2698 expr=1
F.conv2d                 F.conv2d_55              7 1 812 weight.67 bias.29 821 822 823 2698 x_squeezed.14 $input=812 $weight=weight.67 $bias=bias.29 $stride=821 $padding=822 $dilation=823 $groups=2698 #812=(1,12,1,1)f32 #weight.67=(288,12,1,1)f32 #bias.29=(288)f32 #x_squeezed.14=(1,288,1,1)f32
F.sigmoid                F.sigmoid_144            1 1 x_squeezed.14 826 $input=x_squeezed.14 #x_squeezed.14=(1,288,1,1)f32 #826=(1,288,1,1)f32
pnnx.Expression          pnnx_expr_1157           2 1 826 796 x0.14 expr=mul(@0,@1) #826=(1,288,1,1)f32 #796=(1,288,16,63)f32 #x0.14=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1156           0 1 830 expr=None
pnnx.Attribute           effnet._blocks.6._project_conv 0 1 weight.69 @data=(48,288,1,1)f32 #weight.69=(48,288,1,1)f32
pnnx.Expression          pnnx_expr_1152           0 1 836 expr=[1,1]
pnnx.Expression          pnnx_expr_1150           0 1 837 expr=[0,0]
pnnx.Expression          pnnx_expr_1147           0 1 838 expr=[1,1]
pnnx.Expression          pnnx_expr_1146           0 1 2708 expr=1
F.conv2d                 F.conv2d_56              7 1 x0.14 weight.69 830 836 837 838 2708 input.55 $input=x0.14 $weight=weight.69 $bias=830 $stride=836 $padding=837 $dilation=838 $groups=2708 #x0.14=(1,288,16,63)f32 #weight.69=(48,288,1,1)f32 #input.55=(1,48,16,63)f32
nn.BatchNorm2d           effnet._blocks.6._bn2    1 1 input.55 841 affine=True eps=1.000000e-03 num_features=48 @bias=(48)f32 @running_mean=(48)f32 @running_var=(48)f32 @weight=(48)f32 #input.55=(1,48,16,63)f32 #841=(1,48,16,63)f32
pnnx.Expression          pnnx_expr_1144           2 1 841 755 842 expr=add(@0,@1) #841=(1,48,16,63)f32 #755=(1,48,16,63)f32 #842=(1,48,16,63)f32
pnnx.Expression          pnnx_expr_1142           0 1 855 expr=None
pnnx.Attribute           effnet._blocks.7._expand_conv 0 1 weight.71 @data=(288,48,1,1)f32 #weight.71=(288,48,1,1)f32
pnnx.Expression          pnnx_expr_1138           0 1 861 expr=[1,1]
pnnx.Expression          pnnx_expr_1136           0 1 862 expr=[0,0]
pnnx.Expression          pnnx_expr_1133           0 1 863 expr=[1,1]
pnnx.Expression          pnnx_expr_1132           0 1 2719 expr=1
F.conv2d                 F.conv2d_57              7 1 842 weight.71 855 861 862 863 2719 input.57 $input=842 $weight=weight.71 $bias=855 $stride=861 $padding=862 $dilation=863 $groups=2719 #842=(1,48,16,63)f32 #weight.71=(288,48,1,1)f32 #input.57=(1,288,16,63)f32
nn.BatchNorm2d           effnet._blocks.7._bn0    1 1 input.57 866 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.57=(1,288,16,63)f32 #866=(1,288,16,63)f32
nn.SiLU                  effnet._blocks.7._swish  1 1 866 867 #866=(1,288,16,63)f32 #867=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1131           0 1 870 expr=None
pnnx.Expression          pnnx_expr_1128           0 1 873 expr=288
pnnx.Attribute           effnet._blocks.7._depthwise_conv 0 1 weight.73 @data=(288,1,5,5)f32 #weight.73=(288,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.7._depthwise_conv.static_padding 1 1 867 876 padding=(2,2,2,2) #867=(1,288,16,63)f32 #876=(1,288,20,67)f32
pnnx.Expression          pnnx_expr_1126           0 1 877 expr=[1,1]
pnnx.Expression          pnnx_expr_1124           0 1 878 expr=[0,0]
pnnx.Expression          pnnx_expr_1121           0 1 879 expr=[1,1]
F.conv2d                 F.conv2d_58              7 1 876 weight.73 870 877 878 879 873 input.59 $input=876 $weight=weight.73 $bias=870 $stride=877 $padding=878 $dilation=879 $groups=873 #876=(1,288,20,67)f32 #weight.73=(288,1,5,5)f32 #input.59=(1,288,16,63)f32
nn.BatchNorm2d           effnet._blocks.7._bn1    1 1 input.59 882 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.59=(1,288,16,63)f32 #882=(1,288,16,63)f32
nn.SiLU                  pnnx_unique_26           1 1 882 883 #882=(1,288,16,63)f32 #883=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1119           0 1 884 expr=[1,1]
pnnx.Attribute           effnet._blocks.7._se_reduce 0 1 bias.31 @data=(12)f32 #bias.31=(12)f32
pnnx.Attribute           pnnx_unique_27           0 1 weight.75 @data=(12,288,1,1)f32 #weight.75=(12,288,1,1)f32
pnnx.Expression          pnnx_expr_1115           0 1 894 expr=[1,1]
pnnx.Expression          pnnx_expr_1113           0 1 895 expr=[0,0]
pnnx.Expression          pnnx_expr_1110           0 1 896 expr=[1,1]
pnnx.Expression          pnnx_expr_1109           0 1 2739 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_7  2 1 883 884 x.19 $input=883 $output_size=884 #883=(1,288,16,63)f32 #x.19=(1,288,1,1)f32
F.conv2d                 F.conv2d_59              7 1 x.19 weight.75 bias.31 894 895 896 2739 input.61 $input=x.19 $weight=weight.75 $bias=bias.31 $stride=894 $padding=895 $dilation=896 $groups=2739 #x.19=(1,288,1,1)f32 #weight.75=(12,288,1,1)f32 #bias.31=(12)f32 #input.61=(1,12,1,1)f32
nn.SiLU                  pnnx_unique_28           1 1 input.61 899 #input.61=(1,12,1,1)f32 #899=(1,12,1,1)f32
pnnx.Attribute           effnet._blocks.7._se_expand 0 1 bias.33 @data=(288)f32 #bias.33=(288)f32
pnnx.Attribute           pnnx_unique_29           0 1 weight.77 @data=(288,12,1,1)f32 #weight.77=(288,12,1,1)f32
pnnx.Expression          pnnx_expr_1105           0 1 908 expr=[1,1]
pnnx.Expression          pnnx_expr_1103           0 1 909 expr=[0,0]
pnnx.Expression          pnnx_expr_1100           0 1 910 expr=[1,1]
pnnx.Expression          pnnx_expr_1099           0 1 2749 expr=1
F.conv2d                 F.conv2d_60              7 1 899 weight.77 bias.33 908 909 910 2749 x_squeezed.16 $input=899 $weight=weight.77 $bias=bias.33 $stride=908 $padding=909 $dilation=910 $groups=2749 #899=(1,12,1,1)f32 #weight.77=(288,12,1,1)f32 #bias.33=(288)f32 #x_squeezed.16=(1,288,1,1)f32
F.sigmoid                F.sigmoid_145            1 1 x_squeezed.16 913 $input=x_squeezed.16 #x_squeezed.16=(1,288,1,1)f32 #913=(1,288,1,1)f32
pnnx.Expression          pnnx_expr_1098           2 1 913 883 x0.16 expr=mul(@0,@1) #913=(1,288,1,1)f32 #883=(1,288,16,63)f32 #x0.16=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1097           0 1 917 expr=None
pnnx.Attribute           effnet._blocks.7._project_conv 0 1 weight.79 @data=(48,288,1,1)f32 #weight.79=(48,288,1,1)f32
pnnx.Expression          pnnx_expr_1093           0 1 923 expr=[1,1]
pnnx.Expression          pnnx_expr_1091           0 1 924 expr=[0,0]
pnnx.Expression          pnnx_expr_1088           0 1 925 expr=[1,1]
pnnx.Expression          pnnx_expr_1087           0 1 2759 expr=1
F.conv2d                 F.conv2d_61              7 1 x0.16 weight.79 917 923 924 925 2759 input.63 $input=x0.16 $weight=weight.79 $bias=917 $stride=923 $padding=924 $dilation=925 $groups=2759 #x0.16=(1,288,16,63)f32 #weight.79=(48,288,1,1)f32 #input.63=(1,48,16,63)f32
nn.BatchNorm2d           effnet._blocks.7._bn2    1 1 input.63 928 affine=True eps=1.000000e-03 num_features=48 @bias=(48)f32 @running_mean=(48)f32 @running_var=(48)f32 @weight=(48)f32 #input.63=(1,48,16,63)f32 #928=(1,48,16,63)f32
pnnx.Expression          pnnx_expr_1085           2 1 928 842 929 expr=add(@0,@1) #928=(1,48,16,63)f32 #842=(1,48,16,63)f32 #929=(1,48,16,63)f32
pnnx.Expression          pnnx_expr_1083           0 1 942 expr=None
pnnx.Attribute           effnet._blocks.8._expand_conv 0 1 weight.81 @data=(288,48,1,1)f32 #weight.81=(288,48,1,1)f32
pnnx.Expression          pnnx_expr_1079           0 1 948 expr=[1,1]
pnnx.Expression          pnnx_expr_1077           0 1 949 expr=[0,0]
pnnx.Expression          pnnx_expr_1074           0 1 950 expr=[1,1]
pnnx.Expression          pnnx_expr_1073           0 1 2770 expr=1
F.conv2d                 F.conv2d_62              7 1 929 weight.81 942 948 949 950 2770 input.65 $input=929 $weight=weight.81 $bias=942 $stride=948 $padding=949 $dilation=950 $groups=2770 #929=(1,48,16,63)f32 #weight.81=(288,48,1,1)f32 #input.65=(1,288,16,63)f32
nn.BatchNorm2d           effnet._blocks.8._bn0    1 1 input.65 953 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.65=(1,288,16,63)f32 #953=(1,288,16,63)f32
nn.SiLU                  effnet._blocks.8._swish  1 1 953 954 #953=(1,288,16,63)f32 #954=(1,288,16,63)f32
pnnx.Expression          pnnx_expr_1072           0 1 957 expr=None
pnnx.Expression          pnnx_expr_1068           0 1 961 expr=288
pnnx.Attribute           effnet._blocks.8._depthwise_conv 0 1 weight.83 @data=(288,1,3,3)f32 #weight.83=(288,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.8._depthwise_conv.static_padding 1 1 954 964 padding=(1,1,1,1) #954=(1,288,16,63)f32 #964=(1,288,18,65)f32
pnnx.Expression          pnnx_expr_1066           0 1 965 expr=[2,2]
pnnx.Expression          pnnx_expr_1064           0 1 966 expr=[0,0]
pnnx.Expression          pnnx_expr_1062           0 1 967 expr=[1,1]
F.conv2d                 F.conv2d_63              7 1 964 weight.83 957 965 966 967 961 input.67 $input=964 $weight=weight.83 $bias=957 $stride=965 $padding=966 $dilation=967 $groups=961 #964=(1,288,18,65)f32 #weight.83=(288,1,3,3)f32 #input.67=(1,288,8,32)f32
nn.BatchNorm2d           effnet._blocks.8._bn1    1 1 input.67 970 affine=True eps=1.000000e-03 num_features=288 @bias=(288)f32 @running_mean=(288)f32 @running_var=(288)f32 @weight=(288)f32 #input.67=(1,288,8,32)f32 #970=(1,288,8,32)f32
nn.SiLU                  pnnx_unique_30           1 1 970 971 #970=(1,288,8,32)f32 #971=(1,288,8,32)f32
pnnx.Expression          pnnx_expr_1060           0 1 972 expr=[1,1]
pnnx.Attribute           effnet._blocks.8._se_reduce 0 1 bias.35 @data=(12)f32 #bias.35=(12)f32
pnnx.Attribute           pnnx_unique_31           0 1 weight.85 @data=(12,288,1,1)f32 #weight.85=(12,288,1,1)f32
pnnx.Expression          pnnx_expr_1056           0 1 982 expr=[1,1]
pnnx.Expression          pnnx_expr_1054           0 1 983 expr=[0,0]
pnnx.Expression          pnnx_expr_1051           0 1 984 expr=[1,1]
pnnx.Expression          pnnx_expr_1050           0 1 2789 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_8  2 1 971 972 x.21 $input=971 $output_size=972 #971=(1,288,8,32)f32 #x.21=(1,288,1,1)f32
F.conv2d                 F.conv2d_64              7 1 x.21 weight.85 bias.35 982 983 984 2789 input.69 $input=x.21 $weight=weight.85 $bias=bias.35 $stride=982 $padding=983 $dilation=984 $groups=2789 #x.21=(1,288,1,1)f32 #weight.85=(12,288,1,1)f32 #bias.35=(12)f32 #input.69=(1,12,1,1)f32
nn.SiLU                  pnnx_unique_32           1 1 input.69 987 #input.69=(1,12,1,1)f32 #987=(1,12,1,1)f32
pnnx.Attribute           effnet._blocks.8._se_expand 0 1 bias.37 @data=(288)f32 #bias.37=(288)f32
pnnx.Attribute           pnnx_unique_33           0 1 weight.87 @data=(288,12,1,1)f32 #weight.87=(288,12,1,1)f32
pnnx.Expression          pnnx_expr_1046           0 1 996 expr=[1,1]
pnnx.Expression          pnnx_expr_1044           0 1 997 expr=[0,0]
pnnx.Expression          pnnx_expr_1041           0 1 998 expr=[1,1]
pnnx.Expression          pnnx_expr_1040           0 1 2799 expr=1
F.conv2d                 F.conv2d_65              7 1 987 weight.87 bias.37 996 997 998 2799 x_squeezed.18 $input=987 $weight=weight.87 $bias=bias.37 $stride=996 $padding=997 $dilation=998 $groups=2799 #987=(1,12,1,1)f32 #weight.87=(288,12,1,1)f32 #bias.37=(288)f32 #x_squeezed.18=(1,288,1,1)f32
F.sigmoid                F.sigmoid_146            1 1 x_squeezed.18 1001 $input=x_squeezed.18 #x_squeezed.18=(1,288,1,1)f32 #1001=(1,288,1,1)f32
pnnx.Expression          pnnx_expr_1039           2 1 1001 971 x0.18 expr=mul(@0,@1) #1001=(1,288,1,1)f32 #971=(1,288,8,32)f32 #x0.18=(1,288,8,32)f32
pnnx.Expression          pnnx_expr_1038           0 1 1005 expr=None
pnnx.Attribute           effnet._blocks.8._project_conv 0 1 weight.89 @data=(88,288,1,1)f32 #weight.89=(88,288,1,1)f32
pnnx.Expression          pnnx_expr_1034           0 1 1011 expr=[1,1]
pnnx.Expression          pnnx_expr_1032           0 1 1012 expr=[0,0]
pnnx.Expression          pnnx_expr_1029           0 1 1013 expr=[1,1]
pnnx.Expression          pnnx_expr_1028           0 1 2809 expr=1
F.conv2d                 F.conv2d_66              7 1 x0.18 weight.89 1005 1011 1012 1013 2809 input.71 $input=x0.18 $weight=weight.89 $bias=1005 $stride=1011 $padding=1012 $dilation=1013 $groups=2809 #x0.18=(1,288,8,32)f32 #weight.89=(88,288,1,1)f32 #input.71=(1,88,8,32)f32
nn.BatchNorm2d           effnet._blocks.8._bn2    1 1 input.71 1016 affine=True eps=1.000000e-03 num_features=88 @bias=(88)f32 @running_mean=(88)f32 @running_var=(88)f32 @weight=(88)f32 #input.71=(1,88,8,32)f32 #1016=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_1026           0 1 1029 expr=None
pnnx.Attribute           effnet._blocks.9._expand_conv 0 1 weight.91 @data=(528,88,1,1)f32 #weight.91=(528,88,1,1)f32
pnnx.Expression          pnnx_expr_1022           0 1 1035 expr=[1,1]
pnnx.Expression          pnnx_expr_1020           0 1 1036 expr=[0,0]
pnnx.Expression          pnnx_expr_1017           0 1 1037 expr=[1,1]
pnnx.Expression          pnnx_expr_1016           0 1 2819 expr=1
F.conv2d                 F.conv2d_67              7 1 1016 weight.91 1029 1035 1036 1037 2819 input.73 $input=1016 $weight=weight.91 $bias=1029 $stride=1035 $padding=1036 $dilation=1037 $groups=2819 #1016=(1,88,8,32)f32 #weight.91=(528,88,1,1)f32 #input.73=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.9._bn0    1 1 input.73 1040 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.73=(1,528,8,32)f32 #1040=(1,528,8,32)f32
nn.SiLU                  effnet._blocks.9._swish  1 1 1040 1041 #1040=(1,528,8,32)f32 #1041=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_1015           0 1 1044 expr=None
pnnx.Expression          pnnx_expr_1012           0 1 1047 expr=528
pnnx.Attribute           effnet._blocks.9._depthwise_conv 0 1 weight.93 @data=(528,1,3,3)f32 #weight.93=(528,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.9._depthwise_conv.static_padding 1 1 1041 1050 padding=(1,1,1,1) #1041=(1,528,8,32)f32 #1050=(1,528,10,34)f32
pnnx.Expression          pnnx_expr_1010           0 1 1051 expr=[1,1]
pnnx.Expression          pnnx_expr_1008           0 1 1052 expr=[0,0]
pnnx.Expression          pnnx_expr_1005           0 1 1053 expr=[1,1]
F.conv2d                 F.conv2d_68              7 1 1050 weight.93 1044 1051 1052 1053 1047 input.75 $input=1050 $weight=weight.93 $bias=1044 $stride=1051 $padding=1052 $dilation=1053 $groups=1047 #1050=(1,528,10,34)f32 #weight.93=(528,1,3,3)f32 #input.75=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.9._bn1    1 1 input.75 1056 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.75=(1,528,8,32)f32 #1056=(1,528,8,32)f32
nn.SiLU                  pnnx_unique_34           1 1 1056 1057 #1056=(1,528,8,32)f32 #1057=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_1003           0 1 1058 expr=[1,1]
pnnx.Attribute           effnet._blocks.9._se_reduce 0 1 bias.39 @data=(22)f32 #bias.39=(22)f32
pnnx.Attribute           pnnx_unique_35           0 1 weight.95 @data=(22,528,1,1)f32 #weight.95=(22,528,1,1)f32
pnnx.Expression          pnnx_expr_999            0 1 1068 expr=[1,1]
pnnx.Expression          pnnx_expr_997            0 1 1069 expr=[0,0]
pnnx.Expression          pnnx_expr_994            0 1 1070 expr=[1,1]
pnnx.Expression          pnnx_expr_993            0 1 2839 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_9  2 1 1057 1058 x.23 $input=1057 $output_size=1058 #1057=(1,528,8,32)f32 #x.23=(1,528,1,1)f32
F.conv2d                 F.conv2d_69              7 1 x.23 weight.95 bias.39 1068 1069 1070 2839 input.77 $input=x.23 $weight=weight.95 $bias=bias.39 $stride=1068 $padding=1069 $dilation=1070 $groups=2839 #x.23=(1,528,1,1)f32 #weight.95=(22,528,1,1)f32 #bias.39=(22)f32 #input.77=(1,22,1,1)f32
nn.SiLU                  pnnx_unique_36           1 1 input.77 1073 #input.77=(1,22,1,1)f32 #1073=(1,22,1,1)f32
pnnx.Attribute           effnet._blocks.9._se_expand 0 1 bias.41 @data=(528)f32 #bias.41=(528)f32
pnnx.Attribute           pnnx_unique_37           0 1 weight.97 @data=(528,22,1,1)f32 #weight.97=(528,22,1,1)f32
pnnx.Expression          pnnx_expr_989            0 1 1082 expr=[1,1]
pnnx.Expression          pnnx_expr_987            0 1 1083 expr=[0,0]
pnnx.Expression          pnnx_expr_984            0 1 1084 expr=[1,1]
pnnx.Expression          pnnx_expr_983            0 1 2849 expr=1
F.conv2d                 F.conv2d_70              7 1 1073 weight.97 bias.41 1082 1083 1084 2849 x_squeezed.20 $input=1073 $weight=weight.97 $bias=bias.41 $stride=1082 $padding=1083 $dilation=1084 $groups=2849 #1073=(1,22,1,1)f32 #weight.97=(528,22,1,1)f32 #bias.41=(528)f32 #x_squeezed.20=(1,528,1,1)f32
F.sigmoid                F.sigmoid_147            1 1 x_squeezed.20 1087 $input=x_squeezed.20 #x_squeezed.20=(1,528,1,1)f32 #1087=(1,528,1,1)f32
pnnx.Expression          pnnx_expr_982            2 1 1087 1057 x0.20 expr=mul(@0,@1) #1087=(1,528,1,1)f32 #1057=(1,528,8,32)f32 #x0.20=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_981            0 1 1091 expr=None
pnnx.Attribute           effnet._blocks.9._project_conv 0 1 weight.99 @data=(88,528,1,1)f32 #weight.99=(88,528,1,1)f32
pnnx.Expression          pnnx_expr_977            0 1 1097 expr=[1,1]
pnnx.Expression          pnnx_expr_975            0 1 1098 expr=[0,0]
pnnx.Expression          pnnx_expr_972            0 1 1099 expr=[1,1]
pnnx.Expression          pnnx_expr_971            0 1 2859 expr=1
F.conv2d                 F.conv2d_71              7 1 x0.20 weight.99 1091 1097 1098 1099 2859 input.79 $input=x0.20 $weight=weight.99 $bias=1091 $stride=1097 $padding=1098 $dilation=1099 $groups=2859 #x0.20=(1,528,8,32)f32 #weight.99=(88,528,1,1)f32 #input.79=(1,88,8,32)f32
nn.BatchNorm2d           effnet._blocks.9._bn2    1 1 input.79 1102 affine=True eps=1.000000e-03 num_features=88 @bias=(88)f32 @running_mean=(88)f32 @running_var=(88)f32 @weight=(88)f32 #input.79=(1,88,8,32)f32 #1102=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_969            2 1 1102 1016 1103 expr=add(@0,@1) #1102=(1,88,8,32)f32 #1016=(1,88,8,32)f32 #1103=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_967            0 1 1116 expr=None
pnnx.Attribute           effnet._blocks.10._expand_conv 0 1 weight.101 @data=(528,88,1,1)f32 #weight.101=(528,88,1,1)f32
pnnx.Expression          pnnx_expr_963            0 1 1122 expr=[1,1]
pnnx.Expression          pnnx_expr_961            0 1 1123 expr=[0,0]
pnnx.Expression          pnnx_expr_958            0 1 1124 expr=[1,1]
pnnx.Expression          pnnx_expr_957            0 1 2870 expr=1
F.conv2d                 F.conv2d_72              7 1 1103 weight.101 1116 1122 1123 1124 2870 input.81 $input=1103 $weight=weight.101 $bias=1116 $stride=1122 $padding=1123 $dilation=1124 $groups=2870 #1103=(1,88,8,32)f32 #weight.101=(528,88,1,1)f32 #input.81=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.10._bn0   1 1 input.81 1127 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.81=(1,528,8,32)f32 #1127=(1,528,8,32)f32
nn.SiLU                  effnet._blocks.10._swish 1 1 1127 1128 #1127=(1,528,8,32)f32 #1128=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_956            0 1 1131 expr=None
pnnx.Expression          pnnx_expr_953            0 1 1134 expr=528
pnnx.Attribute           effnet._blocks.10._depthwise_conv 0 1 weight.103 @data=(528,1,3,3)f32 #weight.103=(528,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.10._depthwise_conv.static_padding 1 1 1128 1137 padding=(1,1,1,1) #1128=(1,528,8,32)f32 #1137=(1,528,10,34)f32
pnnx.Expression          pnnx_expr_951            0 1 1138 expr=[1,1]
pnnx.Expression          pnnx_expr_949            0 1 1139 expr=[0,0]
pnnx.Expression          pnnx_expr_946            0 1 1140 expr=[1,1]
F.conv2d                 F.conv2d_73              7 1 1137 weight.103 1131 1138 1139 1140 1134 input.83 $input=1137 $weight=weight.103 $bias=1131 $stride=1138 $padding=1139 $dilation=1140 $groups=1134 #1137=(1,528,10,34)f32 #weight.103=(528,1,3,3)f32 #input.83=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.10._bn1   1 1 input.83 1143 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.83=(1,528,8,32)f32 #1143=(1,528,8,32)f32
nn.SiLU                  pnnx_unique_38           1 1 1143 1144 #1143=(1,528,8,32)f32 #1144=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_944            0 1 1145 expr=[1,1]
pnnx.Attribute           effnet._blocks.10._se_reduce 0 1 bias.43 @data=(22)f32 #bias.43=(22)f32
pnnx.Attribute           pnnx_unique_39           0 1 weight.105 @data=(22,528,1,1)f32 #weight.105=(22,528,1,1)f32
pnnx.Expression          pnnx_expr_940            0 1 1155 expr=[1,1]
pnnx.Expression          pnnx_expr_938            0 1 1156 expr=[0,0]
pnnx.Expression          pnnx_expr_935            0 1 1157 expr=[1,1]
pnnx.Expression          pnnx_expr_934            0 1 2890 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_10 2 1 1144 1145 x.25 $input=1144 $output_size=1145 #1144=(1,528,8,32)f32 #x.25=(1,528,1,1)f32
F.conv2d                 F.conv2d_74              7 1 x.25 weight.105 bias.43 1155 1156 1157 2890 input.85 $input=x.25 $weight=weight.105 $bias=bias.43 $stride=1155 $padding=1156 $dilation=1157 $groups=2890 #x.25=(1,528,1,1)f32 #weight.105=(22,528,1,1)f32 #bias.43=(22)f32 #input.85=(1,22,1,1)f32
nn.SiLU                  pnnx_unique_40           1 1 input.85 1160 #input.85=(1,22,1,1)f32 #1160=(1,22,1,1)f32
pnnx.Attribute           effnet._blocks.10._se_expand 0 1 bias.45 @data=(528)f32 #bias.45=(528)f32
pnnx.Attribute           pnnx_unique_41           0 1 weight.107 @data=(528,22,1,1)f32 #weight.107=(528,22,1,1)f32
pnnx.Expression          pnnx_expr_930            0 1 1169 expr=[1,1]
pnnx.Expression          pnnx_expr_928            0 1 1170 expr=[0,0]
pnnx.Expression          pnnx_expr_925            0 1 1171 expr=[1,1]
pnnx.Expression          pnnx_expr_924            0 1 2900 expr=1
F.conv2d                 F.conv2d_75              7 1 1160 weight.107 bias.45 1169 1170 1171 2900 x_squeezed.22 $input=1160 $weight=weight.107 $bias=bias.45 $stride=1169 $padding=1170 $dilation=1171 $groups=2900 #1160=(1,22,1,1)f32 #weight.107=(528,22,1,1)f32 #bias.45=(528)f32 #x_squeezed.22=(1,528,1,1)f32
F.sigmoid                F.sigmoid_148            1 1 x_squeezed.22 1174 $input=x_squeezed.22 #x_squeezed.22=(1,528,1,1)f32 #1174=(1,528,1,1)f32
pnnx.Expression          pnnx_expr_923            2 1 1174 1144 x0.22 expr=mul(@0,@1) #1174=(1,528,1,1)f32 #1144=(1,528,8,32)f32 #x0.22=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_922            0 1 1178 expr=None
pnnx.Attribute           effnet._blocks.10._project_conv 0 1 weight.109 @data=(88,528,1,1)f32 #weight.109=(88,528,1,1)f32
pnnx.Expression          pnnx_expr_918            0 1 1184 expr=[1,1]
pnnx.Expression          pnnx_expr_916            0 1 1185 expr=[0,0]
pnnx.Expression          pnnx_expr_913            0 1 1186 expr=[1,1]
pnnx.Expression          pnnx_expr_912            0 1 2910 expr=1
F.conv2d                 F.conv2d_76              7 1 x0.22 weight.109 1178 1184 1185 1186 2910 input.87 $input=x0.22 $weight=weight.109 $bias=1178 $stride=1184 $padding=1185 $dilation=1186 $groups=2910 #x0.22=(1,528,8,32)f32 #weight.109=(88,528,1,1)f32 #input.87=(1,88,8,32)f32
nn.BatchNorm2d           effnet._blocks.10._bn2   1 1 input.87 1189 affine=True eps=1.000000e-03 num_features=88 @bias=(88)f32 @running_mean=(88)f32 @running_var=(88)f32 @weight=(88)f32 #input.87=(1,88,8,32)f32 #1189=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_910            2 1 1189 1103 1190 expr=add(@0,@1) #1189=(1,88,8,32)f32 #1103=(1,88,8,32)f32 #1190=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_908            0 1 1203 expr=None
pnnx.Attribute           effnet._blocks.11._expand_conv 0 1 weight.111 @data=(528,88,1,1)f32 #weight.111=(528,88,1,1)f32
pnnx.Expression          pnnx_expr_904            0 1 1209 expr=[1,1]
pnnx.Expression          pnnx_expr_902            0 1 1210 expr=[0,0]
pnnx.Expression          pnnx_expr_899            0 1 1211 expr=[1,1]
pnnx.Expression          pnnx_expr_898            0 1 2921 expr=1
F.conv2d                 F.conv2d_77              7 1 1190 weight.111 1203 1209 1210 1211 2921 input.89 $input=1190 $weight=weight.111 $bias=1203 $stride=1209 $padding=1210 $dilation=1211 $groups=2921 #1190=(1,88,8,32)f32 #weight.111=(528,88,1,1)f32 #input.89=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.11._bn0   1 1 input.89 1214 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.89=(1,528,8,32)f32 #1214=(1,528,8,32)f32
nn.SiLU                  effnet._blocks.11._swish 1 1 1214 1215 #1214=(1,528,8,32)f32 #1215=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_897            0 1 1218 expr=None
pnnx.Expression          pnnx_expr_894            0 1 1221 expr=528
pnnx.Attribute           effnet._blocks.11._depthwise_conv 0 1 weight.113 @data=(528,1,3,3)f32 #weight.113=(528,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.11._depthwise_conv.static_padding 1 1 1215 1224 padding=(1,1,1,1) #1215=(1,528,8,32)f32 #1224=(1,528,10,34)f32
pnnx.Expression          pnnx_expr_892            0 1 1225 expr=[1,1]
pnnx.Expression          pnnx_expr_890            0 1 1226 expr=[0,0]
pnnx.Expression          pnnx_expr_887            0 1 1227 expr=[1,1]
F.conv2d                 F.conv2d_78              7 1 1224 weight.113 1218 1225 1226 1227 1221 input.91 $input=1224 $weight=weight.113 $bias=1218 $stride=1225 $padding=1226 $dilation=1227 $groups=1221 #1224=(1,528,10,34)f32 #weight.113=(528,1,3,3)f32 #input.91=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.11._bn1   1 1 input.91 1230 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.91=(1,528,8,32)f32 #1230=(1,528,8,32)f32
nn.SiLU                  pnnx_unique_42           1 1 1230 1231 #1230=(1,528,8,32)f32 #1231=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_885            0 1 1232 expr=[1,1]
pnnx.Attribute           effnet._blocks.11._se_reduce 0 1 bias.47 @data=(22)f32 #bias.47=(22)f32
pnnx.Attribute           pnnx_unique_43           0 1 weight.115 @data=(22,528,1,1)f32 #weight.115=(22,528,1,1)f32
pnnx.Expression          pnnx_expr_881            0 1 1242 expr=[1,1]
pnnx.Expression          pnnx_expr_879            0 1 1243 expr=[0,0]
pnnx.Expression          pnnx_expr_876            0 1 1244 expr=[1,1]
pnnx.Expression          pnnx_expr_875            0 1 2941 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_11 2 1 1231 1232 x.27 $input=1231 $output_size=1232 #1231=(1,528,8,32)f32 #x.27=(1,528,1,1)f32
F.conv2d                 F.conv2d_79              7 1 x.27 weight.115 bias.47 1242 1243 1244 2941 input.93 $input=x.27 $weight=weight.115 $bias=bias.47 $stride=1242 $padding=1243 $dilation=1244 $groups=2941 #x.27=(1,528,1,1)f32 #weight.115=(22,528,1,1)f32 #bias.47=(22)f32 #input.93=(1,22,1,1)f32
nn.SiLU                  pnnx_unique_44           1 1 input.93 1247 #input.93=(1,22,1,1)f32 #1247=(1,22,1,1)f32
pnnx.Attribute           effnet._blocks.11._se_expand 0 1 bias.49 @data=(528)f32 #bias.49=(528)f32
pnnx.Attribute           pnnx_unique_45           0 1 weight.117 @data=(528,22,1,1)f32 #weight.117=(528,22,1,1)f32
pnnx.Expression          pnnx_expr_871            0 1 1256 expr=[1,1]
pnnx.Expression          pnnx_expr_869            0 1 1257 expr=[0,0]
pnnx.Expression          pnnx_expr_866            0 1 1258 expr=[1,1]
pnnx.Expression          pnnx_expr_865            0 1 2951 expr=1
F.conv2d                 F.conv2d_80              7 1 1247 weight.117 bias.49 1256 1257 1258 2951 x_squeezed.24 $input=1247 $weight=weight.117 $bias=bias.49 $stride=1256 $padding=1257 $dilation=1258 $groups=2951 #1247=(1,22,1,1)f32 #weight.117=(528,22,1,1)f32 #bias.49=(528)f32 #x_squeezed.24=(1,528,1,1)f32
F.sigmoid                F.sigmoid_149            1 1 x_squeezed.24 1261 $input=x_squeezed.24 #x_squeezed.24=(1,528,1,1)f32 #1261=(1,528,1,1)f32
pnnx.Expression          pnnx_expr_864            2 1 1261 1231 x0.24 expr=mul(@0,@1) #1261=(1,528,1,1)f32 #1231=(1,528,8,32)f32 #x0.24=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_863            0 1 1265 expr=None
pnnx.Attribute           effnet._blocks.11._project_conv 0 1 weight.119 @data=(88,528,1,1)f32 #weight.119=(88,528,1,1)f32
pnnx.Expression          pnnx_expr_859            0 1 1271 expr=[1,1]
pnnx.Expression          pnnx_expr_857            0 1 1272 expr=[0,0]
pnnx.Expression          pnnx_expr_854            0 1 1273 expr=[1,1]
pnnx.Expression          pnnx_expr_853            0 1 2961 expr=1
F.conv2d                 F.conv2d_81              7 1 x0.24 weight.119 1265 1271 1272 1273 2961 input.95 $input=x0.24 $weight=weight.119 $bias=1265 $stride=1271 $padding=1272 $dilation=1273 $groups=2961 #x0.24=(1,528,8,32)f32 #weight.119=(88,528,1,1)f32 #input.95=(1,88,8,32)f32
nn.BatchNorm2d           effnet._blocks.11._bn2   1 1 input.95 1276 affine=True eps=1.000000e-03 num_features=88 @bias=(88)f32 @running_mean=(88)f32 @running_var=(88)f32 @weight=(88)f32 #input.95=(1,88,8,32)f32 #1276=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_851            2 1 1276 1190 1277 expr=add(@0,@1) #1276=(1,88,8,32)f32 #1190=(1,88,8,32)f32 #1277=(1,88,8,32)f32
pnnx.Expression          pnnx_expr_849            0 1 1290 expr=None
pnnx.Attribute           effnet._blocks.12._expand_conv 0 1 weight.121 @data=(528,88,1,1)f32 #weight.121=(528,88,1,1)f32
pnnx.Expression          pnnx_expr_845            0 1 1296 expr=[1,1]
pnnx.Expression          pnnx_expr_843            0 1 1297 expr=[0,0]
pnnx.Expression          pnnx_expr_840            0 1 1298 expr=[1,1]
pnnx.Expression          pnnx_expr_839            0 1 2972 expr=1
F.conv2d                 F.conv2d_82              7 1 1277 weight.121 1290 1296 1297 1298 2972 input.97 $input=1277 $weight=weight.121 $bias=1290 $stride=1296 $padding=1297 $dilation=1298 $groups=2972 #1277=(1,88,8,32)f32 #weight.121=(528,88,1,1)f32 #input.97=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.12._bn0   1 1 input.97 1301 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.97=(1,528,8,32)f32 #1301=(1,528,8,32)f32
nn.SiLU                  effnet._blocks.12._swish 1 1 1301 1302 #1301=(1,528,8,32)f32 #1302=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_838            0 1 1305 expr=None
pnnx.Expression          pnnx_expr_835            0 1 1308 expr=528
pnnx.Attribute           effnet._blocks.12._depthwise_conv 0 1 weight.123 @data=(528,1,5,5)f32 #weight.123=(528,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.12._depthwise_conv.static_padding 1 1 1302 1311 padding=(2,2,2,2) #1302=(1,528,8,32)f32 #1311=(1,528,12,36)f32
pnnx.Expression          pnnx_expr_833            0 1 1312 expr=[1,1]
pnnx.Expression          pnnx_expr_831            0 1 1313 expr=[0,0]
pnnx.Expression          pnnx_expr_828            0 1 1314 expr=[1,1]
F.conv2d                 F.conv2d_83              7 1 1311 weight.123 1305 1312 1313 1314 1308 input.99 $input=1311 $weight=weight.123 $bias=1305 $stride=1312 $padding=1313 $dilation=1314 $groups=1308 #1311=(1,528,12,36)f32 #weight.123=(528,1,5,5)f32 #input.99=(1,528,8,32)f32
nn.BatchNorm2d           effnet._blocks.12._bn1   1 1 input.99 1317 affine=True eps=1.000000e-03 num_features=528 @bias=(528)f32 @running_mean=(528)f32 @running_var=(528)f32 @weight=(528)f32 #input.99=(1,528,8,32)f32 #1317=(1,528,8,32)f32
nn.SiLU                  pnnx_unique_46           1 1 1317 1318 #1317=(1,528,8,32)f32 #1318=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_826            0 1 1319 expr=[1,1]
pnnx.Attribute           effnet._blocks.12._se_reduce 0 1 bias.51 @data=(22)f32 #bias.51=(22)f32
pnnx.Attribute           pnnx_unique_47           0 1 weight.125 @data=(22,528,1,1)f32 #weight.125=(22,528,1,1)f32
pnnx.Expression          pnnx_expr_822            0 1 1329 expr=[1,1]
pnnx.Expression          pnnx_expr_820            0 1 1330 expr=[0,0]
pnnx.Expression          pnnx_expr_817            0 1 1331 expr=[1,1]
pnnx.Expression          pnnx_expr_816            0 1 2992 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_12 2 1 1318 1319 x.29 $input=1318 $output_size=1319 #1318=(1,528,8,32)f32 #x.29=(1,528,1,1)f32
F.conv2d                 F.conv2d_84              7 1 x.29 weight.125 bias.51 1329 1330 1331 2992 input.101 $input=x.29 $weight=weight.125 $bias=bias.51 $stride=1329 $padding=1330 $dilation=1331 $groups=2992 #x.29=(1,528,1,1)f32 #weight.125=(22,528,1,1)f32 #bias.51=(22)f32 #input.101=(1,22,1,1)f32
nn.SiLU                  pnnx_unique_48           1 1 input.101 1334 #input.101=(1,22,1,1)f32 #1334=(1,22,1,1)f32
pnnx.Attribute           effnet._blocks.12._se_expand 0 1 bias.53 @data=(528)f32 #bias.53=(528)f32
pnnx.Attribute           pnnx_unique_49           0 1 weight.127 @data=(528,22,1,1)f32 #weight.127=(528,22,1,1)f32
pnnx.Expression          pnnx_expr_812            0 1 1343 expr=[1,1]
pnnx.Expression          pnnx_expr_810            0 1 1344 expr=[0,0]
pnnx.Expression          pnnx_expr_807            0 1 1345 expr=[1,1]
pnnx.Expression          pnnx_expr_806            0 1 3002 expr=1
F.conv2d                 F.conv2d_85              7 1 1334 weight.127 bias.53 1343 1344 1345 3002 x_squeezed.26 $input=1334 $weight=weight.127 $bias=bias.53 $stride=1343 $padding=1344 $dilation=1345 $groups=3002 #1334=(1,22,1,1)f32 #weight.127=(528,22,1,1)f32 #bias.53=(528)f32 #x_squeezed.26=(1,528,1,1)f32
F.sigmoid                F.sigmoid_150            1 1 x_squeezed.26 1348 $input=x_squeezed.26 #x_squeezed.26=(1,528,1,1)f32 #1348=(1,528,1,1)f32
pnnx.Expression          pnnx_expr_805            2 1 1348 1318 x0.26 expr=mul(@0,@1) #1348=(1,528,1,1)f32 #1318=(1,528,8,32)f32 #x0.26=(1,528,8,32)f32
pnnx.Expression          pnnx_expr_804            0 1 1352 expr=None
pnnx.Attribute           effnet._blocks.12._project_conv 0 1 weight.129 @data=(120,528,1,1)f32 #weight.129=(120,528,1,1)f32
pnnx.Expression          pnnx_expr_800            0 1 1358 expr=[1,1]
pnnx.Expression          pnnx_expr_798            0 1 1359 expr=[0,0]
pnnx.Expression          pnnx_expr_795            0 1 1360 expr=[1,1]
pnnx.Expression          pnnx_expr_794            0 1 3012 expr=1
F.conv2d                 F.conv2d_86              7 1 x0.26 weight.129 1352 1358 1359 1360 3012 input.103 $input=x0.26 $weight=weight.129 $bias=1352 $stride=1358 $padding=1359 $dilation=1360 $groups=3012 #x0.26=(1,528,8,32)f32 #weight.129=(120,528,1,1)f32 #input.103=(1,120,8,32)f32
nn.BatchNorm2d           effnet._blocks.12._bn2   1 1 input.103 1363 affine=True eps=1.000000e-03 num_features=120 @bias=(120)f32 @running_mean=(120)f32 @running_var=(120)f32 @weight=(120)f32 #input.103=(1,120,8,32)f32 #1363=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_792            0 1 1376 expr=None
pnnx.Attribute           effnet._blocks.13._expand_conv 0 1 weight.131 @data=(720,120,1,1)f32 #weight.131=(720,120,1,1)f32
pnnx.Expression          pnnx_expr_788            0 1 1382 expr=[1,1]
pnnx.Expression          pnnx_expr_786            0 1 1383 expr=[0,0]
pnnx.Expression          pnnx_expr_783            0 1 1384 expr=[1,1]
pnnx.Expression          pnnx_expr_782            0 1 3022 expr=1
F.conv2d                 F.conv2d_87              7 1 1363 weight.131 1376 1382 1383 1384 3022 input.105 $input=1363 $weight=weight.131 $bias=1376 $stride=1382 $padding=1383 $dilation=1384 $groups=3022 #1363=(1,120,8,32)f32 #weight.131=(720,120,1,1)f32 #input.105=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.13._bn0   1 1 input.105 1387 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.105=(1,720,8,32)f32 #1387=(1,720,8,32)f32
nn.SiLU                  effnet._blocks.13._swish 1 1 1387 1388 #1387=(1,720,8,32)f32 #1388=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_781            0 1 1391 expr=None
pnnx.Expression          pnnx_expr_778            0 1 1394 expr=720
pnnx.Attribute           effnet._blocks.13._depthwise_conv 0 1 weight.133 @data=(720,1,5,5)f32 #weight.133=(720,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.13._depthwise_conv.static_padding 1 1 1388 1397 padding=(2,2,2,2) #1388=(1,720,8,32)f32 #1397=(1,720,12,36)f32
pnnx.Expression          pnnx_expr_776            0 1 1398 expr=[1,1]
pnnx.Expression          pnnx_expr_774            0 1 1399 expr=[0,0]
pnnx.Expression          pnnx_expr_771            0 1 1400 expr=[1,1]
F.conv2d                 F.conv2d_88              7 1 1397 weight.133 1391 1398 1399 1400 1394 input.107 $input=1397 $weight=weight.133 $bias=1391 $stride=1398 $padding=1399 $dilation=1400 $groups=1394 #1397=(1,720,12,36)f32 #weight.133=(720,1,5,5)f32 #input.107=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.13._bn1   1 1 input.107 1403 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.107=(1,720,8,32)f32 #1403=(1,720,8,32)f32
nn.SiLU                  pnnx_unique_50           1 1 1403 1404 #1403=(1,720,8,32)f32 #1404=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_769            0 1 1405 expr=[1,1]
pnnx.Attribute           effnet._blocks.13._se_reduce 0 1 bias.55 @data=(30)f32 #bias.55=(30)f32
pnnx.Attribute           pnnx_unique_51           0 1 weight.135 @data=(30,720,1,1)f32 #weight.135=(30,720,1,1)f32
pnnx.Expression          pnnx_expr_765            0 1 1415 expr=[1,1]
pnnx.Expression          pnnx_expr_763            0 1 1416 expr=[0,0]
pnnx.Expression          pnnx_expr_760            0 1 1417 expr=[1,1]
pnnx.Expression          pnnx_expr_759            0 1 3042 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_13 2 1 1404 1405 x.31 $input=1404 $output_size=1405 #1404=(1,720,8,32)f32 #x.31=(1,720,1,1)f32
F.conv2d                 F.conv2d_89              7 1 x.31 weight.135 bias.55 1415 1416 1417 3042 input.109 $input=x.31 $weight=weight.135 $bias=bias.55 $stride=1415 $padding=1416 $dilation=1417 $groups=3042 #x.31=(1,720,1,1)f32 #weight.135=(30,720,1,1)f32 #bias.55=(30)f32 #input.109=(1,30,1,1)f32
nn.SiLU                  pnnx_unique_52           1 1 input.109 1420 #input.109=(1,30,1,1)f32 #1420=(1,30,1,1)f32
pnnx.Attribute           effnet._blocks.13._se_expand 0 1 bias.57 @data=(720)f32 #bias.57=(720)f32
pnnx.Attribute           pnnx_unique_53           0 1 weight.137 @data=(720,30,1,1)f32 #weight.137=(720,30,1,1)f32
pnnx.Expression          pnnx_expr_755            0 1 1429 expr=[1,1]
pnnx.Expression          pnnx_expr_753            0 1 1430 expr=[0,0]
pnnx.Expression          pnnx_expr_750            0 1 1431 expr=[1,1]
pnnx.Expression          pnnx_expr_749            0 1 3052 expr=1
F.conv2d                 F.conv2d_90              7 1 1420 weight.137 bias.57 1429 1430 1431 3052 x_squeezed.28 $input=1420 $weight=weight.137 $bias=bias.57 $stride=1429 $padding=1430 $dilation=1431 $groups=3052 #1420=(1,30,1,1)f32 #weight.137=(720,30,1,1)f32 #bias.57=(720)f32 #x_squeezed.28=(1,720,1,1)f32
F.sigmoid                F.sigmoid_151            1 1 x_squeezed.28 1434 $input=x_squeezed.28 #x_squeezed.28=(1,720,1,1)f32 #1434=(1,720,1,1)f32
pnnx.Expression          pnnx_expr_748            2 1 1434 1404 x0.28 expr=mul(@0,@1) #1434=(1,720,1,1)f32 #1404=(1,720,8,32)f32 #x0.28=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_747            0 1 1438 expr=None
pnnx.Attribute           effnet._blocks.13._project_conv 0 1 weight.139 @data=(120,720,1,1)f32 #weight.139=(120,720,1,1)f32
pnnx.Expression          pnnx_expr_743            0 1 1444 expr=[1,1]
pnnx.Expression          pnnx_expr_741            0 1 1445 expr=[0,0]
pnnx.Expression          pnnx_expr_738            0 1 1446 expr=[1,1]
pnnx.Expression          pnnx_expr_737            0 1 3062 expr=1
F.conv2d                 F.conv2d_91              7 1 x0.28 weight.139 1438 1444 1445 1446 3062 input.111 $input=x0.28 $weight=weight.139 $bias=1438 $stride=1444 $padding=1445 $dilation=1446 $groups=3062 #x0.28=(1,720,8,32)f32 #weight.139=(120,720,1,1)f32 #input.111=(1,120,8,32)f32
nn.BatchNorm2d           effnet._blocks.13._bn2   1 1 input.111 1449 affine=True eps=1.000000e-03 num_features=120 @bias=(120)f32 @running_mean=(120)f32 @running_var=(120)f32 @weight=(120)f32 #input.111=(1,120,8,32)f32 #1449=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_735            2 1 1449 1363 1450 expr=add(@0,@1) #1449=(1,120,8,32)f32 #1363=(1,120,8,32)f32 #1450=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_733            0 1 1463 expr=None
pnnx.Attribute           effnet._blocks.14._expand_conv 0 1 weight.141 @data=(720,120,1,1)f32 #weight.141=(720,120,1,1)f32
pnnx.Expression          pnnx_expr_729            0 1 1469 expr=[1,1]
pnnx.Expression          pnnx_expr_727            0 1 1470 expr=[0,0]
pnnx.Expression          pnnx_expr_724            0 1 1471 expr=[1,1]
pnnx.Expression          pnnx_expr_723            0 1 3073 expr=1
F.conv2d                 F.conv2d_92              7 1 1450 weight.141 1463 1469 1470 1471 3073 input.113 $input=1450 $weight=weight.141 $bias=1463 $stride=1469 $padding=1470 $dilation=1471 $groups=3073 #1450=(1,120,8,32)f32 #weight.141=(720,120,1,1)f32 #input.113=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.14._bn0   1 1 input.113 1474 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.113=(1,720,8,32)f32 #1474=(1,720,8,32)f32
nn.SiLU                  effnet._blocks.14._swish 1 1 1474 1475 #1474=(1,720,8,32)f32 #1475=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_722            0 1 1478 expr=None
pnnx.Expression          pnnx_expr_719            0 1 1481 expr=720
pnnx.Attribute           effnet._blocks.14._depthwise_conv 0 1 weight.143 @data=(720,1,5,5)f32 #weight.143=(720,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.14._depthwise_conv.static_padding 1 1 1475 1484 padding=(2,2,2,2) #1475=(1,720,8,32)f32 #1484=(1,720,12,36)f32
pnnx.Expression          pnnx_expr_717            0 1 1485 expr=[1,1]
pnnx.Expression          pnnx_expr_715            0 1 1486 expr=[0,0]
pnnx.Expression          pnnx_expr_712            0 1 1487 expr=[1,1]
F.conv2d                 F.conv2d_93              7 1 1484 weight.143 1478 1485 1486 1487 1481 input.115 $input=1484 $weight=weight.143 $bias=1478 $stride=1485 $padding=1486 $dilation=1487 $groups=1481 #1484=(1,720,12,36)f32 #weight.143=(720,1,5,5)f32 #input.115=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.14._bn1   1 1 input.115 1490 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.115=(1,720,8,32)f32 #1490=(1,720,8,32)f32
nn.SiLU                  pnnx_unique_54           1 1 1490 1491 #1490=(1,720,8,32)f32 #1491=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_710            0 1 1492 expr=[1,1]
pnnx.Attribute           effnet._blocks.14._se_reduce 0 1 bias.59 @data=(30)f32 #bias.59=(30)f32
pnnx.Attribute           pnnx_unique_55           0 1 weight.145 @data=(30,720,1,1)f32 #weight.145=(30,720,1,1)f32
pnnx.Expression          pnnx_expr_706            0 1 1502 expr=[1,1]
pnnx.Expression          pnnx_expr_704            0 1 1503 expr=[0,0]
pnnx.Expression          pnnx_expr_701            0 1 1504 expr=[1,1]
pnnx.Expression          pnnx_expr_700            0 1 3093 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_14 2 1 1491 1492 x.33 $input=1491 $output_size=1492 #1491=(1,720,8,32)f32 #x.33=(1,720,1,1)f32
F.conv2d                 F.conv2d_94              7 1 x.33 weight.145 bias.59 1502 1503 1504 3093 input.117 $input=x.33 $weight=weight.145 $bias=bias.59 $stride=1502 $padding=1503 $dilation=1504 $groups=3093 #x.33=(1,720,1,1)f32 #weight.145=(30,720,1,1)f32 #bias.59=(30)f32 #input.117=(1,30,1,1)f32
nn.SiLU                  pnnx_unique_56           1 1 input.117 1507 #input.117=(1,30,1,1)f32 #1507=(1,30,1,1)f32
pnnx.Attribute           effnet._blocks.14._se_expand 0 1 bias.61 @data=(720)f32 #bias.61=(720)f32
pnnx.Attribute           pnnx_unique_57           0 1 weight.147 @data=(720,30,1,1)f32 #weight.147=(720,30,1,1)f32
pnnx.Expression          pnnx_expr_696            0 1 1516 expr=[1,1]
pnnx.Expression          pnnx_expr_694            0 1 1517 expr=[0,0]
pnnx.Expression          pnnx_expr_691            0 1 1518 expr=[1,1]
pnnx.Expression          pnnx_expr_690            0 1 3103 expr=1
F.conv2d                 F.conv2d_95              7 1 1507 weight.147 bias.61 1516 1517 1518 3103 x_squeezed.30 $input=1507 $weight=weight.147 $bias=bias.61 $stride=1516 $padding=1517 $dilation=1518 $groups=3103 #1507=(1,30,1,1)f32 #weight.147=(720,30,1,1)f32 #bias.61=(720)f32 #x_squeezed.30=(1,720,1,1)f32
F.sigmoid                F.sigmoid_152            1 1 x_squeezed.30 1521 $input=x_squeezed.30 #x_squeezed.30=(1,720,1,1)f32 #1521=(1,720,1,1)f32
pnnx.Expression          pnnx_expr_689            2 1 1521 1491 x0.30 expr=mul(@0,@1) #1521=(1,720,1,1)f32 #1491=(1,720,8,32)f32 #x0.30=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_688            0 1 1525 expr=None
pnnx.Attribute           effnet._blocks.14._project_conv 0 1 weight.149 @data=(120,720,1,1)f32 #weight.149=(120,720,1,1)f32
pnnx.Expression          pnnx_expr_684            0 1 1531 expr=[1,1]
pnnx.Expression          pnnx_expr_682            0 1 1532 expr=[0,0]
pnnx.Expression          pnnx_expr_679            0 1 1533 expr=[1,1]
pnnx.Expression          pnnx_expr_678            0 1 3113 expr=1
F.conv2d                 F.conv2d_96              7 1 x0.30 weight.149 1525 1531 1532 1533 3113 input.119 $input=x0.30 $weight=weight.149 $bias=1525 $stride=1531 $padding=1532 $dilation=1533 $groups=3113 #x0.30=(1,720,8,32)f32 #weight.149=(120,720,1,1)f32 #input.119=(1,120,8,32)f32
nn.BatchNorm2d           effnet._blocks.14._bn2   1 1 input.119 1536 affine=True eps=1.000000e-03 num_features=120 @bias=(120)f32 @running_mean=(120)f32 @running_var=(120)f32 @weight=(120)f32 #input.119=(1,120,8,32)f32 #1536=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_676            2 1 1536 1450 1537 expr=add(@0,@1) #1536=(1,120,8,32)f32 #1450=(1,120,8,32)f32 #1537=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_674            0 1 1550 expr=None
pnnx.Attribute           effnet._blocks.15._expand_conv 0 1 weight.151 @data=(720,120,1,1)f32 #weight.151=(720,120,1,1)f32
pnnx.Expression          pnnx_expr_670            0 1 1556 expr=[1,1]
pnnx.Expression          pnnx_expr_668            0 1 1557 expr=[0,0]
pnnx.Expression          pnnx_expr_665            0 1 1558 expr=[1,1]
pnnx.Expression          pnnx_expr_664            0 1 3124 expr=1
F.conv2d                 F.conv2d_97              7 1 1537 weight.151 1550 1556 1557 1558 3124 input.121 $input=1537 $weight=weight.151 $bias=1550 $stride=1556 $padding=1557 $dilation=1558 $groups=3124 #1537=(1,120,8,32)f32 #weight.151=(720,120,1,1)f32 #input.121=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.15._bn0   1 1 input.121 1561 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.121=(1,720,8,32)f32 #1561=(1,720,8,32)f32
nn.SiLU                  effnet._blocks.15._swish 1 1 1561 1562 #1561=(1,720,8,32)f32 #1562=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_663            0 1 1565 expr=None
pnnx.Expression          pnnx_expr_660            0 1 1568 expr=720
pnnx.Attribute           effnet._blocks.15._depthwise_conv 0 1 weight.153 @data=(720,1,5,5)f32 #weight.153=(720,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.15._depthwise_conv.static_padding 1 1 1562 1571 padding=(2,2,2,2) #1562=(1,720,8,32)f32 #1571=(1,720,12,36)f32
pnnx.Expression          pnnx_expr_658            0 1 1572 expr=[1,1]
pnnx.Expression          pnnx_expr_656            0 1 1573 expr=[0,0]
pnnx.Expression          pnnx_expr_653            0 1 1574 expr=[1,1]
F.conv2d                 F.conv2d_98              7 1 1571 weight.153 1565 1572 1573 1574 1568 input.123 $input=1571 $weight=weight.153 $bias=1565 $stride=1572 $padding=1573 $dilation=1574 $groups=1568 #1571=(1,720,12,36)f32 #weight.153=(720,1,5,5)f32 #input.123=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.15._bn1   1 1 input.123 1577 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.123=(1,720,8,32)f32 #1577=(1,720,8,32)f32
nn.SiLU                  pnnx_unique_58           1 1 1577 1578 #1577=(1,720,8,32)f32 #1578=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_651            0 1 1579 expr=[1,1]
pnnx.Attribute           effnet._blocks.15._se_reduce 0 1 bias.63 @data=(30)f32 #bias.63=(30)f32
pnnx.Attribute           pnnx_unique_59           0 1 weight.155 @data=(30,720,1,1)f32 #weight.155=(30,720,1,1)f32
pnnx.Expression          pnnx_expr_647            0 1 1589 expr=[1,1]
pnnx.Expression          pnnx_expr_645            0 1 1590 expr=[0,0]
pnnx.Expression          pnnx_expr_642            0 1 1591 expr=[1,1]
pnnx.Expression          pnnx_expr_641            0 1 3144 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_15 2 1 1578 1579 x.35 $input=1578 $output_size=1579 #1578=(1,720,8,32)f32 #x.35=(1,720,1,1)f32
F.conv2d                 F.conv2d_99              7 1 x.35 weight.155 bias.63 1589 1590 1591 3144 input.125 $input=x.35 $weight=weight.155 $bias=bias.63 $stride=1589 $padding=1590 $dilation=1591 $groups=3144 #x.35=(1,720,1,1)f32 #weight.155=(30,720,1,1)f32 #bias.63=(30)f32 #input.125=(1,30,1,1)f32
nn.SiLU                  pnnx_unique_60           1 1 input.125 1594 #input.125=(1,30,1,1)f32 #1594=(1,30,1,1)f32
pnnx.Attribute           effnet._blocks.15._se_expand 0 1 bias.65 @data=(720)f32 #bias.65=(720)f32
pnnx.Attribute           pnnx_unique_61           0 1 weight.157 @data=(720,30,1,1)f32 #weight.157=(720,30,1,1)f32
pnnx.Expression          pnnx_expr_637            0 1 1603 expr=[1,1]
pnnx.Expression          pnnx_expr_635            0 1 1604 expr=[0,0]
pnnx.Expression          pnnx_expr_632            0 1 1605 expr=[1,1]
pnnx.Expression          pnnx_expr_631            0 1 3154 expr=1
F.conv2d                 F.conv2d_100             7 1 1594 weight.157 bias.65 1603 1604 1605 3154 x_squeezed.32 $input=1594 $weight=weight.157 $bias=bias.65 $stride=1603 $padding=1604 $dilation=1605 $groups=3154 #1594=(1,30,1,1)f32 #weight.157=(720,30,1,1)f32 #bias.65=(720)f32 #x_squeezed.32=(1,720,1,1)f32
F.sigmoid                F.sigmoid_153            1 1 x_squeezed.32 1608 $input=x_squeezed.32 #x_squeezed.32=(1,720,1,1)f32 #1608=(1,720,1,1)f32
pnnx.Expression          pnnx_expr_630            2 1 1608 1578 x0.32 expr=mul(@0,@1) #1608=(1,720,1,1)f32 #1578=(1,720,8,32)f32 #x0.32=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_629            0 1 1612 expr=None
pnnx.Attribute           effnet._blocks.15._project_conv 0 1 weight.159 @data=(120,720,1,1)f32 #weight.159=(120,720,1,1)f32
pnnx.Expression          pnnx_expr_625            0 1 1618 expr=[1,1]
pnnx.Expression          pnnx_expr_623            0 1 1619 expr=[0,0]
pnnx.Expression          pnnx_expr_620            0 1 1620 expr=[1,1]
pnnx.Expression          pnnx_expr_619            0 1 3164 expr=1
F.conv2d                 F.conv2d_101             7 1 x0.32 weight.159 1612 1618 1619 1620 3164 input.127 $input=x0.32 $weight=weight.159 $bias=1612 $stride=1618 $padding=1619 $dilation=1620 $groups=3164 #x0.32=(1,720,8,32)f32 #weight.159=(120,720,1,1)f32 #input.127=(1,120,8,32)f32
nn.BatchNorm2d           effnet._blocks.15._bn2   1 1 input.127 1623 affine=True eps=1.000000e-03 num_features=120 @bias=(120)f32 @running_mean=(120)f32 @running_var=(120)f32 @weight=(120)f32 #input.127=(1,120,8,32)f32 #1623=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_617            2 1 1623 1537 1624 expr=add(@0,@1) #1623=(1,120,8,32)f32 #1537=(1,120,8,32)f32 #1624=(1,120,8,32)f32
pnnx.Expression          pnnx_expr_615            0 1 1637 expr=None
pnnx.Attribute           effnet._blocks.16._expand_conv 0 1 weight.161 @data=(720,120,1,1)f32 #weight.161=(720,120,1,1)f32
pnnx.Expression          pnnx_expr_611            0 1 1643 expr=[1,1]
pnnx.Expression          pnnx_expr_609            0 1 1644 expr=[0,0]
pnnx.Expression          pnnx_expr_606            0 1 1645 expr=[1,1]
pnnx.Expression          pnnx_expr_605            0 1 3175 expr=1
F.conv2d                 F.conv2d_102             7 1 1624 weight.161 1637 1643 1644 1645 3175 input.129 $input=1624 $weight=weight.161 $bias=1637 $stride=1643 $padding=1644 $dilation=1645 $groups=3175 #1624=(1,120,8,32)f32 #weight.161=(720,120,1,1)f32 #input.129=(1,720,8,32)f32
nn.BatchNorm2d           effnet._blocks.16._bn0   1 1 input.129 1648 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.129=(1,720,8,32)f32 #1648=(1,720,8,32)f32
nn.SiLU                  effnet._blocks.16._swish 1 1 1648 1649 #1648=(1,720,8,32)f32 #1649=(1,720,8,32)f32
pnnx.Expression          pnnx_expr_604            0 1 1652 expr=None
pnnx.Expression          pnnx_expr_600            0 1 1656 expr=720
pnnx.Attribute           effnet._blocks.16._depthwise_conv 0 1 weight.163 @data=(720,1,5,5)f32 #weight.163=(720,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.16._depthwise_conv.static_padding 1 1 1649 1659 padding=(2,2,2,2) #1649=(1,720,8,32)f32 #1659=(1,720,12,36)f32
pnnx.Expression          pnnx_expr_598            0 1 1660 expr=[2,2]
pnnx.Expression          pnnx_expr_596            0 1 1661 expr=[0,0]
pnnx.Expression          pnnx_expr_594            0 1 1662 expr=[1,1]
F.conv2d                 F.conv2d_103             7 1 1659 weight.163 1652 1660 1661 1662 1656 input.131 $input=1659 $weight=weight.163 $bias=1652 $stride=1660 $padding=1661 $dilation=1662 $groups=1656 #1659=(1,720,12,36)f32 #weight.163=(720,1,5,5)f32 #input.131=(1,720,4,16)f32
nn.BatchNorm2d           effnet._blocks.16._bn1   1 1 input.131 1665 affine=True eps=1.000000e-03 num_features=720 @bias=(720)f32 @running_mean=(720)f32 @running_var=(720)f32 @weight=(720)f32 #input.131=(1,720,4,16)f32 #1665=(1,720,4,16)f32
nn.SiLU                  pnnx_unique_62           1 1 1665 1666 #1665=(1,720,4,16)f32 #1666=(1,720,4,16)f32
pnnx.Expression          pnnx_expr_592            0 1 1667 expr=[1,1]
pnnx.Attribute           effnet._blocks.16._se_reduce 0 1 bias.67 @data=(30)f32 #bias.67=(30)f32
pnnx.Attribute           pnnx_unique_63           0 1 weight.165 @data=(30,720,1,1)f32 #weight.165=(30,720,1,1)f32
pnnx.Expression          pnnx_expr_588            0 1 1677 expr=[1,1]
pnnx.Expression          pnnx_expr_586            0 1 1678 expr=[0,0]
pnnx.Expression          pnnx_expr_583            0 1 1679 expr=[1,1]
pnnx.Expression          pnnx_expr_582            0 1 3194 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_16 2 1 1666 1667 x.37 $input=1666 $output_size=1667 #1666=(1,720,4,16)f32 #x.37=(1,720,1,1)f32
F.conv2d                 F.conv2d_104             7 1 x.37 weight.165 bias.67 1677 1678 1679 3194 input.133 $input=x.37 $weight=weight.165 $bias=bias.67 $stride=1677 $padding=1678 $dilation=1679 $groups=3194 #x.37=(1,720,1,1)f32 #weight.165=(30,720,1,1)f32 #bias.67=(30)f32 #input.133=(1,30,1,1)f32
nn.SiLU                  pnnx_unique_64           1 1 input.133 1682 #input.133=(1,30,1,1)f32 #1682=(1,30,1,1)f32
pnnx.Attribute           effnet._blocks.16._se_expand 0 1 bias.69 @data=(720)f32 #bias.69=(720)f32
pnnx.Attribute           pnnx_unique_65           0 1 weight.167 @data=(720,30,1,1)f32 #weight.167=(720,30,1,1)f32
pnnx.Expression          pnnx_expr_578            0 1 1691 expr=[1,1]
pnnx.Expression          pnnx_expr_576            0 1 1692 expr=[0,0]
pnnx.Expression          pnnx_expr_573            0 1 1693 expr=[1,1]
pnnx.Expression          pnnx_expr_572            0 1 3204 expr=1
F.conv2d                 F.conv2d_105             7 1 1682 weight.167 bias.69 1691 1692 1693 3204 x_squeezed.34 $input=1682 $weight=weight.167 $bias=bias.69 $stride=1691 $padding=1692 $dilation=1693 $groups=3204 #1682=(1,30,1,1)f32 #weight.167=(720,30,1,1)f32 #bias.69=(720)f32 #x_squeezed.34=(1,720,1,1)f32
F.sigmoid                F.sigmoid_154            1 1 x_squeezed.34 1696 $input=x_squeezed.34 #x_squeezed.34=(1,720,1,1)f32 #1696=(1,720,1,1)f32
pnnx.Expression          pnnx_expr_571            2 1 1696 1666 x0.34 expr=mul(@0,@1) #1696=(1,720,1,1)f32 #1666=(1,720,4,16)f32 #x0.34=(1,720,4,16)f32
pnnx.Expression          pnnx_expr_570            0 1 1700 expr=None
pnnx.Attribute           effnet._blocks.16._project_conv 0 1 weight.169 @data=(208,720,1,1)f32 #weight.169=(208,720,1,1)f32
pnnx.Expression          pnnx_expr_566            0 1 1706 expr=[1,1]
pnnx.Expression          pnnx_expr_564            0 1 1707 expr=[0,0]
pnnx.Expression          pnnx_expr_561            0 1 1708 expr=[1,1]
pnnx.Expression          pnnx_expr_560            0 1 3214 expr=1
F.conv2d                 F.conv2d_106             7 1 x0.34 weight.169 1700 1706 1707 1708 3214 input.135 $input=x0.34 $weight=weight.169 $bias=1700 $stride=1706 $padding=1707 $dilation=1708 $groups=3214 #x0.34=(1,720,4,16)f32 #weight.169=(208,720,1,1)f32 #input.135=(1,208,4,16)f32
nn.BatchNorm2d           effnet._blocks.16._bn2   1 1 input.135 1711 affine=True eps=1.000000e-03 num_features=208 @bias=(208)f32 @running_mean=(208)f32 @running_var=(208)f32 @weight=(208)f32 #input.135=(1,208,4,16)f32 #1711=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_558            0 1 1724 expr=None
pnnx.Attribute           effnet._blocks.17._expand_conv 0 1 weight.171 @data=(1248,208,1,1)f32 #weight.171=(1248,208,1,1)f32
pnnx.Expression          pnnx_expr_554            0 1 1730 expr=[1,1]
pnnx.Expression          pnnx_expr_552            0 1 1731 expr=[0,0]
pnnx.Expression          pnnx_expr_549            0 1 1732 expr=[1,1]
pnnx.Expression          pnnx_expr_548            0 1 3224 expr=1
F.conv2d                 F.conv2d_107             7 1 1711 weight.171 1724 1730 1731 1732 3224 input.137 $input=1711 $weight=weight.171 $bias=1724 $stride=1730 $padding=1731 $dilation=1732 $groups=3224 #1711=(1,208,4,16)f32 #weight.171=(1248,208,1,1)f32 #input.137=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.17._bn0   1 1 input.137 1735 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.137=(1,1248,4,16)f32 #1735=(1,1248,4,16)f32
nn.SiLU                  effnet._blocks.17._swish 1 1 1735 1736 #1735=(1,1248,4,16)f32 #1736=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_547            0 1 1739 expr=None
pnnx.Expression          pnnx_expr_544            0 1 1742 expr=1248
pnnx.Attribute           effnet._blocks.17._depthwise_conv 0 1 weight.173 @data=(1248,1,5,5)f32 #weight.173=(1248,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.17._depthwise_conv.static_padding 1 1 1736 1745 padding=(2,2,2,2) #1736=(1,1248,4,16)f32 #1745=(1,1248,8,20)f32
pnnx.Expression          pnnx_expr_542            0 1 1746 expr=[1,1]
pnnx.Expression          pnnx_expr_540            0 1 1747 expr=[0,0]
pnnx.Expression          pnnx_expr_537            0 1 1748 expr=[1,1]
F.conv2d                 F.conv2d_108             7 1 1745 weight.173 1739 1746 1747 1748 1742 input.139 $input=1745 $weight=weight.173 $bias=1739 $stride=1746 $padding=1747 $dilation=1748 $groups=1742 #1745=(1,1248,8,20)f32 #weight.173=(1248,1,5,5)f32 #input.139=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.17._bn1   1 1 input.139 1751 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.139=(1,1248,4,16)f32 #1751=(1,1248,4,16)f32
nn.SiLU                  pnnx_unique_66           1 1 1751 1752 #1751=(1,1248,4,16)f32 #1752=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_535            0 1 1753 expr=[1,1]
pnnx.Attribute           effnet._blocks.17._se_reduce 0 1 bias.71 @data=(52)f32 #bias.71=(52)f32
pnnx.Attribute           pnnx_unique_67           0 1 weight.175 @data=(52,1248,1,1)f32 #weight.175=(52,1248,1,1)f32
pnnx.Expression          pnnx_expr_531            0 1 1763 expr=[1,1]
pnnx.Expression          pnnx_expr_529            0 1 1764 expr=[0,0]
pnnx.Expression          pnnx_expr_526            0 1 1765 expr=[1,1]
pnnx.Expression          pnnx_expr_525            0 1 3244 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_17 2 1 1752 1753 x.39 $input=1752 $output_size=1753 #1752=(1,1248,4,16)f32 #x.39=(1,1248,1,1)f32
F.conv2d                 F.conv2d_109             7 1 x.39 weight.175 bias.71 1763 1764 1765 3244 input.141 $input=x.39 $weight=weight.175 $bias=bias.71 $stride=1763 $padding=1764 $dilation=1765 $groups=3244 #x.39=(1,1248,1,1)f32 #weight.175=(52,1248,1,1)f32 #bias.71=(52)f32 #input.141=(1,52,1,1)f32
nn.SiLU                  pnnx_unique_68           1 1 input.141 1768 #input.141=(1,52,1,1)f32 #1768=(1,52,1,1)f32
pnnx.Attribute           effnet._blocks.17._se_expand 0 1 bias.73 @data=(1248)f32 #bias.73=(1248)f32
pnnx.Attribute           pnnx_unique_69           0 1 weight.177 @data=(1248,52,1,1)f32 #weight.177=(1248,52,1,1)f32
pnnx.Expression          pnnx_expr_521            0 1 1777 expr=[1,1]
pnnx.Expression          pnnx_expr_519            0 1 1778 expr=[0,0]
pnnx.Expression          pnnx_expr_516            0 1 1779 expr=[1,1]
pnnx.Expression          pnnx_expr_515            0 1 3254 expr=1
F.conv2d                 F.conv2d_110             7 1 1768 weight.177 bias.73 1777 1778 1779 3254 x_squeezed.36 $input=1768 $weight=weight.177 $bias=bias.73 $stride=1777 $padding=1778 $dilation=1779 $groups=3254 #1768=(1,52,1,1)f32 #weight.177=(1248,52,1,1)f32 #bias.73=(1248)f32 #x_squeezed.36=(1,1248,1,1)f32
F.sigmoid                F.sigmoid_155            1 1 x_squeezed.36 1782 $input=x_squeezed.36 #x_squeezed.36=(1,1248,1,1)f32 #1782=(1,1248,1,1)f32
pnnx.Expression          pnnx_expr_514            2 1 1782 1752 x0.36 expr=mul(@0,@1) #1782=(1,1248,1,1)f32 #1752=(1,1248,4,16)f32 #x0.36=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_513            0 1 1786 expr=None
pnnx.Attribute           effnet._blocks.17._project_conv 0 1 weight.179 @data=(208,1248,1,1)f32 #weight.179=(208,1248,1,1)f32
pnnx.Expression          pnnx_expr_509            0 1 1792 expr=[1,1]
pnnx.Expression          pnnx_expr_507            0 1 1793 expr=[0,0]
pnnx.Expression          pnnx_expr_504            0 1 1794 expr=[1,1]
pnnx.Expression          pnnx_expr_503            0 1 3264 expr=1
F.conv2d                 F.conv2d_111             7 1 x0.36 weight.179 1786 1792 1793 1794 3264 input.143 $input=x0.36 $weight=weight.179 $bias=1786 $stride=1792 $padding=1793 $dilation=1794 $groups=3264 #x0.36=(1,1248,4,16)f32 #weight.179=(208,1248,1,1)f32 #input.143=(1,208,4,16)f32
nn.BatchNorm2d           effnet._blocks.17._bn2   1 1 input.143 1797 affine=True eps=1.000000e-03 num_features=208 @bias=(208)f32 @running_mean=(208)f32 @running_var=(208)f32 @weight=(208)f32 #input.143=(1,208,4,16)f32 #1797=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_501            2 1 1797 1711 1798 expr=add(@0,@1) #1797=(1,208,4,16)f32 #1711=(1,208,4,16)f32 #1798=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_499            0 1 1811 expr=None
pnnx.Attribute           effnet._blocks.18._expand_conv 0 1 weight.181 @data=(1248,208,1,1)f32 #weight.181=(1248,208,1,1)f32
pnnx.Expression          pnnx_expr_495            0 1 1817 expr=[1,1]
pnnx.Expression          pnnx_expr_493            0 1 1818 expr=[0,0]
pnnx.Expression          pnnx_expr_490            0 1 1819 expr=[1,1]
pnnx.Expression          pnnx_expr_489            0 1 3275 expr=1
F.conv2d                 F.conv2d_112             7 1 1798 weight.181 1811 1817 1818 1819 3275 input.145 $input=1798 $weight=weight.181 $bias=1811 $stride=1817 $padding=1818 $dilation=1819 $groups=3275 #1798=(1,208,4,16)f32 #weight.181=(1248,208,1,1)f32 #input.145=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.18._bn0   1 1 input.145 1822 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.145=(1,1248,4,16)f32 #1822=(1,1248,4,16)f32
nn.SiLU                  effnet._blocks.18._swish 1 1 1822 1823 #1822=(1,1248,4,16)f32 #1823=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_488            0 1 1826 expr=None
pnnx.Expression          pnnx_expr_485            0 1 1829 expr=1248
pnnx.Attribute           effnet._blocks.18._depthwise_conv 0 1 weight.183 @data=(1248,1,5,5)f32 #weight.183=(1248,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.18._depthwise_conv.static_padding 1 1 1823 1832 padding=(2,2,2,2) #1823=(1,1248,4,16)f32 #1832=(1,1248,8,20)f32
pnnx.Expression          pnnx_expr_483            0 1 1833 expr=[1,1]
pnnx.Expression          pnnx_expr_481            0 1 1834 expr=[0,0]
pnnx.Expression          pnnx_expr_478            0 1 1835 expr=[1,1]
F.conv2d                 F.conv2d_113             7 1 1832 weight.183 1826 1833 1834 1835 1829 input.147 $input=1832 $weight=weight.183 $bias=1826 $stride=1833 $padding=1834 $dilation=1835 $groups=1829 #1832=(1,1248,8,20)f32 #weight.183=(1248,1,5,5)f32 #input.147=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.18._bn1   1 1 input.147 1838 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.147=(1,1248,4,16)f32 #1838=(1,1248,4,16)f32
nn.SiLU                  pnnx_unique_70           1 1 1838 1839 #1838=(1,1248,4,16)f32 #1839=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_476            0 1 1840 expr=[1,1]
pnnx.Attribute           effnet._blocks.18._se_reduce 0 1 bias.75 @data=(52)f32 #bias.75=(52)f32
pnnx.Attribute           pnnx_unique_71           0 1 weight.185 @data=(52,1248,1,1)f32 #weight.185=(52,1248,1,1)f32
pnnx.Expression          pnnx_expr_472            0 1 1850 expr=[1,1]
pnnx.Expression          pnnx_expr_470            0 1 1851 expr=[0,0]
pnnx.Expression          pnnx_expr_467            0 1 1852 expr=[1,1]
pnnx.Expression          pnnx_expr_466            0 1 3295 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_18 2 1 1839 1840 x.41 $input=1839 $output_size=1840 #1839=(1,1248,4,16)f32 #x.41=(1,1248,1,1)f32
F.conv2d                 F.conv2d_114             7 1 x.41 weight.185 bias.75 1850 1851 1852 3295 input.149 $input=x.41 $weight=weight.185 $bias=bias.75 $stride=1850 $padding=1851 $dilation=1852 $groups=3295 #x.41=(1,1248,1,1)f32 #weight.185=(52,1248,1,1)f32 #bias.75=(52)f32 #input.149=(1,52,1,1)f32
nn.SiLU                  pnnx_unique_72           1 1 input.149 1855 #input.149=(1,52,1,1)f32 #1855=(1,52,1,1)f32
pnnx.Attribute           effnet._blocks.18._se_expand 0 1 bias.77 @data=(1248)f32 #bias.77=(1248)f32
pnnx.Attribute           pnnx_unique_73           0 1 weight.187 @data=(1248,52,1,1)f32 #weight.187=(1248,52,1,1)f32
pnnx.Expression          pnnx_expr_462            0 1 1864 expr=[1,1]
pnnx.Expression          pnnx_expr_460            0 1 1865 expr=[0,0]
pnnx.Expression          pnnx_expr_457            0 1 1866 expr=[1,1]
pnnx.Expression          pnnx_expr_456            0 1 3305 expr=1
F.conv2d                 F.conv2d_115             7 1 1855 weight.187 bias.77 1864 1865 1866 3305 x_squeezed.38 $input=1855 $weight=weight.187 $bias=bias.77 $stride=1864 $padding=1865 $dilation=1866 $groups=3305 #1855=(1,52,1,1)f32 #weight.187=(1248,52,1,1)f32 #bias.77=(1248)f32 #x_squeezed.38=(1,1248,1,1)f32
F.sigmoid                F.sigmoid_156            1 1 x_squeezed.38 1869 $input=x_squeezed.38 #x_squeezed.38=(1,1248,1,1)f32 #1869=(1,1248,1,1)f32
pnnx.Expression          pnnx_expr_455            2 1 1869 1839 x0.38 expr=mul(@0,@1) #1869=(1,1248,1,1)f32 #1839=(1,1248,4,16)f32 #x0.38=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_454            0 1 1873 expr=None
pnnx.Attribute           effnet._blocks.18._project_conv 0 1 weight.189 @data=(208,1248,1,1)f32 #weight.189=(208,1248,1,1)f32
pnnx.Expression          pnnx_expr_450            0 1 1879 expr=[1,1]
pnnx.Expression          pnnx_expr_448            0 1 1880 expr=[0,0]
pnnx.Expression          pnnx_expr_445            0 1 1881 expr=[1,1]
pnnx.Expression          pnnx_expr_444            0 1 3315 expr=1
F.conv2d                 F.conv2d_116             7 1 x0.38 weight.189 1873 1879 1880 1881 3315 input.151 $input=x0.38 $weight=weight.189 $bias=1873 $stride=1879 $padding=1880 $dilation=1881 $groups=3315 #x0.38=(1,1248,4,16)f32 #weight.189=(208,1248,1,1)f32 #input.151=(1,208,4,16)f32
nn.BatchNorm2d           effnet._blocks.18._bn2   1 1 input.151 1884 affine=True eps=1.000000e-03 num_features=208 @bias=(208)f32 @running_mean=(208)f32 @running_var=(208)f32 @weight=(208)f32 #input.151=(1,208,4,16)f32 #1884=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_442            2 1 1884 1798 1885 expr=add(@0,@1) #1884=(1,208,4,16)f32 #1798=(1,208,4,16)f32 #1885=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_440            0 1 1898 expr=None
pnnx.Attribute           effnet._blocks.19._expand_conv 0 1 weight.191 @data=(1248,208,1,1)f32 #weight.191=(1248,208,1,1)f32
pnnx.Expression          pnnx_expr_436            0 1 1904 expr=[1,1]
pnnx.Expression          pnnx_expr_434            0 1 1905 expr=[0,0]
pnnx.Expression          pnnx_expr_431            0 1 1906 expr=[1,1]
pnnx.Expression          pnnx_expr_430            0 1 3326 expr=1
F.conv2d                 F.conv2d_117             7 1 1885 weight.191 1898 1904 1905 1906 3326 input.153 $input=1885 $weight=weight.191 $bias=1898 $stride=1904 $padding=1905 $dilation=1906 $groups=3326 #1885=(1,208,4,16)f32 #weight.191=(1248,208,1,1)f32 #input.153=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.19._bn0   1 1 input.153 1909 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.153=(1,1248,4,16)f32 #1909=(1,1248,4,16)f32
nn.SiLU                  effnet._blocks.19._swish 1 1 1909 1910 #1909=(1,1248,4,16)f32 #1910=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_429            0 1 1913 expr=None
pnnx.Expression          pnnx_expr_426            0 1 1916 expr=1248
pnnx.Attribute           effnet._blocks.19._depthwise_conv 0 1 weight.193 @data=(1248,1,5,5)f32 #weight.193=(1248,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.19._depthwise_conv.static_padding 1 1 1910 1919 padding=(2,2,2,2) #1910=(1,1248,4,16)f32 #1919=(1,1248,8,20)f32
pnnx.Expression          pnnx_expr_424            0 1 1920 expr=[1,1]
pnnx.Expression          pnnx_expr_422            0 1 1921 expr=[0,0]
pnnx.Expression          pnnx_expr_419            0 1 1922 expr=[1,1]
F.conv2d                 F.conv2d_118             7 1 1919 weight.193 1913 1920 1921 1922 1916 input.155 $input=1919 $weight=weight.193 $bias=1913 $stride=1920 $padding=1921 $dilation=1922 $groups=1916 #1919=(1,1248,8,20)f32 #weight.193=(1248,1,5,5)f32 #input.155=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.19._bn1   1 1 input.155 1925 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.155=(1,1248,4,16)f32 #1925=(1,1248,4,16)f32
nn.SiLU                  pnnx_unique_74           1 1 1925 1926 #1925=(1,1248,4,16)f32 #1926=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_417            0 1 1927 expr=[1,1]
pnnx.Attribute           effnet._blocks.19._se_reduce 0 1 bias.79 @data=(52)f32 #bias.79=(52)f32
pnnx.Attribute           pnnx_unique_75           0 1 weight.195 @data=(52,1248,1,1)f32 #weight.195=(52,1248,1,1)f32
pnnx.Expression          pnnx_expr_413            0 1 1937 expr=[1,1]
pnnx.Expression          pnnx_expr_411            0 1 1938 expr=[0,0]
pnnx.Expression          pnnx_expr_408            0 1 1939 expr=[1,1]
pnnx.Expression          pnnx_expr_407            0 1 3346 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_19 2 1 1926 1927 x.43 $input=1926 $output_size=1927 #1926=(1,1248,4,16)f32 #x.43=(1,1248,1,1)f32
F.conv2d                 F.conv2d_119             7 1 x.43 weight.195 bias.79 1937 1938 1939 3346 input.157 $input=x.43 $weight=weight.195 $bias=bias.79 $stride=1937 $padding=1938 $dilation=1939 $groups=3346 #x.43=(1,1248,1,1)f32 #weight.195=(52,1248,1,1)f32 #bias.79=(52)f32 #input.157=(1,52,1,1)f32
nn.SiLU                  pnnx_unique_76           1 1 input.157 1942 #input.157=(1,52,1,1)f32 #1942=(1,52,1,1)f32
pnnx.Attribute           effnet._blocks.19._se_expand 0 1 bias.81 @data=(1248)f32 #bias.81=(1248)f32
pnnx.Attribute           pnnx_unique_77           0 1 weight.197 @data=(1248,52,1,1)f32 #weight.197=(1248,52,1,1)f32
pnnx.Expression          pnnx_expr_403            0 1 1951 expr=[1,1]
pnnx.Expression          pnnx_expr_401            0 1 1952 expr=[0,0]
pnnx.Expression          pnnx_expr_398            0 1 1953 expr=[1,1]
pnnx.Expression          pnnx_expr_397            0 1 3356 expr=1
F.conv2d                 F.conv2d_120             7 1 1942 weight.197 bias.81 1951 1952 1953 3356 x_squeezed.40 $input=1942 $weight=weight.197 $bias=bias.81 $stride=1951 $padding=1952 $dilation=1953 $groups=3356 #1942=(1,52,1,1)f32 #weight.197=(1248,52,1,1)f32 #bias.81=(1248)f32 #x_squeezed.40=(1,1248,1,1)f32
F.sigmoid                F.sigmoid_157            1 1 x_squeezed.40 1956 $input=x_squeezed.40 #x_squeezed.40=(1,1248,1,1)f32 #1956=(1,1248,1,1)f32
pnnx.Expression          pnnx_expr_396            2 1 1956 1926 x0.40 expr=mul(@0,@1) #1956=(1,1248,1,1)f32 #1926=(1,1248,4,16)f32 #x0.40=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_395            0 1 1960 expr=None
pnnx.Attribute           effnet._blocks.19._project_conv 0 1 weight.199 @data=(208,1248,1,1)f32 #weight.199=(208,1248,1,1)f32
pnnx.Expression          pnnx_expr_391            0 1 1966 expr=[1,1]
pnnx.Expression          pnnx_expr_389            0 1 1967 expr=[0,0]
pnnx.Expression          pnnx_expr_386            0 1 1968 expr=[1,1]
pnnx.Expression          pnnx_expr_385            0 1 3366 expr=1
F.conv2d                 F.conv2d_121             7 1 x0.40 weight.199 1960 1966 1967 1968 3366 input.159 $input=x0.40 $weight=weight.199 $bias=1960 $stride=1966 $padding=1967 $dilation=1968 $groups=3366 #x0.40=(1,1248,4,16)f32 #weight.199=(208,1248,1,1)f32 #input.159=(1,208,4,16)f32
nn.BatchNorm2d           effnet._blocks.19._bn2   1 1 input.159 1971 affine=True eps=1.000000e-03 num_features=208 @bias=(208)f32 @running_mean=(208)f32 @running_var=(208)f32 @weight=(208)f32 #input.159=(1,208,4,16)f32 #1971=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_383            2 1 1971 1885 1972 expr=add(@0,@1) #1971=(1,208,4,16)f32 #1885=(1,208,4,16)f32 #1972=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_381            0 1 1985 expr=None
pnnx.Attribute           effnet._blocks.20._expand_conv 0 1 weight.201 @data=(1248,208,1,1)f32 #weight.201=(1248,208,1,1)f32
pnnx.Expression          pnnx_expr_377            0 1 1991 expr=[1,1]
pnnx.Expression          pnnx_expr_375            0 1 1992 expr=[0,0]
pnnx.Expression          pnnx_expr_372            0 1 1993 expr=[1,1]
pnnx.Expression          pnnx_expr_371            0 1 3377 expr=1
F.conv2d                 F.conv2d_122             7 1 1972 weight.201 1985 1991 1992 1993 3377 input.161 $input=1972 $weight=weight.201 $bias=1985 $stride=1991 $padding=1992 $dilation=1993 $groups=3377 #1972=(1,208,4,16)f32 #weight.201=(1248,208,1,1)f32 #input.161=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.20._bn0   1 1 input.161 1996 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.161=(1,1248,4,16)f32 #1996=(1,1248,4,16)f32
nn.SiLU                  effnet._blocks.20._swish 1 1 1996 1997 #1996=(1,1248,4,16)f32 #1997=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_370            0 1 2000 expr=None
pnnx.Expression          pnnx_expr_367            0 1 2003 expr=1248
pnnx.Attribute           effnet._blocks.20._depthwise_conv 0 1 weight.203 @data=(1248,1,5,5)f32 #weight.203=(1248,1,5,5)f32
nn.ZeroPad2d             effnet._blocks.20._depthwise_conv.static_padding 1 1 1997 2006 padding=(2,2,2,2) #1997=(1,1248,4,16)f32 #2006=(1,1248,8,20)f32
pnnx.Expression          pnnx_expr_365            0 1 2007 expr=[1,1]
pnnx.Expression          pnnx_expr_363            0 1 2008 expr=[0,0]
pnnx.Expression          pnnx_expr_360            0 1 2009 expr=[1,1]
F.conv2d                 F.conv2d_123             7 1 2006 weight.203 2000 2007 2008 2009 2003 input.163 $input=2006 $weight=weight.203 $bias=2000 $stride=2007 $padding=2008 $dilation=2009 $groups=2003 #2006=(1,1248,8,20)f32 #weight.203=(1248,1,5,5)f32 #input.163=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.20._bn1   1 1 input.163 2012 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.163=(1,1248,4,16)f32 #2012=(1,1248,4,16)f32
nn.SiLU                  pnnx_unique_78           1 1 2012 2013 #2012=(1,1248,4,16)f32 #2013=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_358            0 1 2014 expr=[1,1]
pnnx.Attribute           effnet._blocks.20._se_reduce 0 1 bias.83 @data=(52)f32 #bias.83=(52)f32
pnnx.Attribute           pnnx_unique_79           0 1 weight.205 @data=(52,1248,1,1)f32 #weight.205=(52,1248,1,1)f32
pnnx.Expression          pnnx_expr_354            0 1 2024 expr=[1,1]
pnnx.Expression          pnnx_expr_352            0 1 2025 expr=[0,0]
pnnx.Expression          pnnx_expr_349            0 1 2026 expr=[1,1]
pnnx.Expression          pnnx_expr_348            0 1 3397 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_20 2 1 2013 2014 x.45 $input=2013 $output_size=2014 #2013=(1,1248,4,16)f32 #x.45=(1,1248,1,1)f32
F.conv2d                 F.conv2d_124             7 1 x.45 weight.205 bias.83 2024 2025 2026 3397 input.165 $input=x.45 $weight=weight.205 $bias=bias.83 $stride=2024 $padding=2025 $dilation=2026 $groups=3397 #x.45=(1,1248,1,1)f32 #weight.205=(52,1248,1,1)f32 #bias.83=(52)f32 #input.165=(1,52,1,1)f32
nn.SiLU                  pnnx_unique_80           1 1 input.165 2029 #input.165=(1,52,1,1)f32 #2029=(1,52,1,1)f32
pnnx.Attribute           effnet._blocks.20._se_expand 0 1 bias.85 @data=(1248)f32 #bias.85=(1248)f32
pnnx.Attribute           pnnx_unique_81           0 1 weight.207 @data=(1248,52,1,1)f32 #weight.207=(1248,52,1,1)f32
pnnx.Expression          pnnx_expr_344            0 1 2038 expr=[1,1]
pnnx.Expression          pnnx_expr_342            0 1 2039 expr=[0,0]
pnnx.Expression          pnnx_expr_339            0 1 2040 expr=[1,1]
pnnx.Expression          pnnx_expr_338            0 1 3407 expr=1
F.conv2d                 F.conv2d_125             7 1 2029 weight.207 bias.85 2038 2039 2040 3407 x_squeezed.42 $input=2029 $weight=weight.207 $bias=bias.85 $stride=2038 $padding=2039 $dilation=2040 $groups=3407 #2029=(1,52,1,1)f32 #weight.207=(1248,52,1,1)f32 #bias.85=(1248)f32 #x_squeezed.42=(1,1248,1,1)f32
F.sigmoid                F.sigmoid_158            1 1 x_squeezed.42 2043 $input=x_squeezed.42 #x_squeezed.42=(1,1248,1,1)f32 #2043=(1,1248,1,1)f32
pnnx.Expression          pnnx_expr_337            2 1 2043 2013 x0.42 expr=mul(@0,@1) #2043=(1,1248,1,1)f32 #2013=(1,1248,4,16)f32 #x0.42=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_336            0 1 2047 expr=None
pnnx.Attribute           effnet._blocks.20._project_conv 0 1 weight.209 @data=(208,1248,1,1)f32 #weight.209=(208,1248,1,1)f32
pnnx.Expression          pnnx_expr_332            0 1 2053 expr=[1,1]
pnnx.Expression          pnnx_expr_330            0 1 2054 expr=[0,0]
pnnx.Expression          pnnx_expr_327            0 1 2055 expr=[1,1]
pnnx.Expression          pnnx_expr_326            0 1 3417 expr=1
F.conv2d                 F.conv2d_126             7 1 x0.42 weight.209 2047 2053 2054 2055 3417 input.167 $input=x0.42 $weight=weight.209 $bias=2047 $stride=2053 $padding=2054 $dilation=2055 $groups=3417 #x0.42=(1,1248,4,16)f32 #weight.209=(208,1248,1,1)f32 #input.167=(1,208,4,16)f32
nn.BatchNorm2d           effnet._blocks.20._bn2   1 1 input.167 2058 affine=True eps=1.000000e-03 num_features=208 @bias=(208)f32 @running_mean=(208)f32 @running_var=(208)f32 @weight=(208)f32 #input.167=(1,208,4,16)f32 #2058=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_324            2 1 2058 1972 2059 expr=add(@0,@1) #2058=(1,208,4,16)f32 #1972=(1,208,4,16)f32 #2059=(1,208,4,16)f32
pnnx.Expression          pnnx_expr_322            0 1 2072 expr=None
pnnx.Attribute           effnet._blocks.21._expand_conv 0 1 weight.211 @data=(1248,208,1,1)f32 #weight.211=(1248,208,1,1)f32
pnnx.Expression          pnnx_expr_318            0 1 2078 expr=[1,1]
pnnx.Expression          pnnx_expr_316            0 1 2079 expr=[0,0]
pnnx.Expression          pnnx_expr_313            0 1 2080 expr=[1,1]
pnnx.Expression          pnnx_expr_312            0 1 3428 expr=1
F.conv2d                 F.conv2d_127             7 1 2059 weight.211 2072 2078 2079 2080 3428 input.169 $input=2059 $weight=weight.211 $bias=2072 $stride=2078 $padding=2079 $dilation=2080 $groups=3428 #2059=(1,208,4,16)f32 #weight.211=(1248,208,1,1)f32 #input.169=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.21._bn0   1 1 input.169 2083 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.169=(1,1248,4,16)f32 #2083=(1,1248,4,16)f32
nn.SiLU                  effnet._blocks.21._swish 1 1 2083 2084 #2083=(1,1248,4,16)f32 #2084=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_311            0 1 2087 expr=None
pnnx.Expression          pnnx_expr_308            0 1 2090 expr=1248
pnnx.Attribute           effnet._blocks.21._depthwise_conv 0 1 weight.213 @data=(1248,1,3,3)f32 #weight.213=(1248,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.21._depthwise_conv.static_padding 1 1 2084 2093 padding=(1,1,1,1) #2084=(1,1248,4,16)f32 #2093=(1,1248,6,18)f32
pnnx.Expression          pnnx_expr_306            0 1 2094 expr=[1,1]
pnnx.Expression          pnnx_expr_304            0 1 2095 expr=[0,0]
pnnx.Expression          pnnx_expr_301            0 1 2096 expr=[1,1]
F.conv2d                 F.conv2d_128             7 1 2093 weight.213 2087 2094 2095 2096 2090 input.171 $input=2093 $weight=weight.213 $bias=2087 $stride=2094 $padding=2095 $dilation=2096 $groups=2090 #2093=(1,1248,6,18)f32 #weight.213=(1248,1,3,3)f32 #input.171=(1,1248,4,16)f32
nn.BatchNorm2d           effnet._blocks.21._bn1   1 1 input.171 2099 affine=True eps=1.000000e-03 num_features=1248 @bias=(1248)f32 @running_mean=(1248)f32 @running_var=(1248)f32 @weight=(1248)f32 #input.171=(1,1248,4,16)f32 #2099=(1,1248,4,16)f32
nn.SiLU                  pnnx_unique_82           1 1 2099 2100 #2099=(1,1248,4,16)f32 #2100=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_299            0 1 2101 expr=[1,1]
pnnx.Attribute           effnet._blocks.21._se_reduce 0 1 bias.87 @data=(52)f32 #bias.87=(52)f32
pnnx.Attribute           pnnx_unique_83           0 1 weight.215 @data=(52,1248,1,1)f32 #weight.215=(52,1248,1,1)f32
pnnx.Expression          pnnx_expr_295            0 1 2111 expr=[1,1]
pnnx.Expression          pnnx_expr_293            0 1 2112 expr=[0,0]
pnnx.Expression          pnnx_expr_290            0 1 2113 expr=[1,1]
pnnx.Expression          pnnx_expr_289            0 1 3448 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_21 2 1 2100 2101 x.47 $input=2100 $output_size=2101 #2100=(1,1248,4,16)f32 #x.47=(1,1248,1,1)f32
F.conv2d                 F.conv2d_129             7 1 x.47 weight.215 bias.87 2111 2112 2113 3448 input.173 $input=x.47 $weight=weight.215 $bias=bias.87 $stride=2111 $padding=2112 $dilation=2113 $groups=3448 #x.47=(1,1248,1,1)f32 #weight.215=(52,1248,1,1)f32 #bias.87=(52)f32 #input.173=(1,52,1,1)f32
nn.SiLU                  pnnx_unique_84           1 1 input.173 2116 #input.173=(1,52,1,1)f32 #2116=(1,52,1,1)f32
pnnx.Attribute           effnet._blocks.21._se_expand 0 1 bias.89 @data=(1248)f32 #bias.89=(1248)f32
pnnx.Attribute           pnnx_unique_85           0 1 weight.217 @data=(1248,52,1,1)f32 #weight.217=(1248,52,1,1)f32
pnnx.Expression          pnnx_expr_285            0 1 2125 expr=[1,1]
pnnx.Expression          pnnx_expr_283            0 1 2126 expr=[0,0]
pnnx.Expression          pnnx_expr_280            0 1 2127 expr=[1,1]
pnnx.Expression          pnnx_expr_279            0 1 3458 expr=1
F.conv2d                 F.conv2d_130             7 1 2116 weight.217 bias.89 2125 2126 2127 3458 x_squeezed.44 $input=2116 $weight=weight.217 $bias=bias.89 $stride=2125 $padding=2126 $dilation=2127 $groups=3458 #2116=(1,52,1,1)f32 #weight.217=(1248,52,1,1)f32 #bias.89=(1248)f32 #x_squeezed.44=(1,1248,1,1)f32
F.sigmoid                F.sigmoid_159            1 1 x_squeezed.44 2130 $input=x_squeezed.44 #x_squeezed.44=(1,1248,1,1)f32 #2130=(1,1248,1,1)f32
pnnx.Expression          pnnx_expr_278            2 1 2130 2100 x0.44 expr=mul(@0,@1) #2130=(1,1248,1,1)f32 #2100=(1,1248,4,16)f32 #x0.44=(1,1248,4,16)f32
pnnx.Expression          pnnx_expr_277            0 1 2134 expr=None
pnnx.Attribute           effnet._blocks.21._project_conv 0 1 weight.219 @data=(352,1248,1,1)f32 #weight.219=(352,1248,1,1)f32
pnnx.Expression          pnnx_expr_273            0 1 2140 expr=[1,1]
pnnx.Expression          pnnx_expr_271            0 1 2141 expr=[0,0]
pnnx.Expression          pnnx_expr_268            0 1 2142 expr=[1,1]
pnnx.Expression          pnnx_expr_267            0 1 3468 expr=1
F.conv2d                 F.conv2d_131             7 1 x0.44 weight.219 2134 2140 2141 2142 3468 input.175 $input=x0.44 $weight=weight.219 $bias=2134 $stride=2140 $padding=2141 $dilation=2142 $groups=3468 #x0.44=(1,1248,4,16)f32 #weight.219=(352,1248,1,1)f32 #input.175=(1,352,4,16)f32
nn.BatchNorm2d           effnet._blocks.21._bn2   1 1 input.175 2145 affine=True eps=1.000000e-03 num_features=352 @bias=(352)f32 @running_mean=(352)f32 @running_var=(352)f32 @weight=(352)f32 #input.175=(1,352,4,16)f32 #2145=(1,352,4,16)f32
pnnx.Expression          pnnx_expr_265            0 1 2158 expr=None
pnnx.Attribute           effnet._blocks.22._expand_conv 0 1 weight.2 @data=(2112,352,1,1)f32 #weight.2=(2112,352,1,1)f32
pnnx.Expression          pnnx_expr_261            0 1 2164 expr=[1,1]
pnnx.Expression          pnnx_expr_259            0 1 2165 expr=[0,0]
pnnx.Expression          pnnx_expr_256            0 1 2166 expr=[1,1]
pnnx.Expression          pnnx_expr_255            0 1 3478 expr=1
F.conv2d                 F.conv2d_132             7 1 2145 weight.2 2158 2164 2165 2166 3478 input.2 $input=2145 $weight=weight.2 $bias=2158 $stride=2164 $padding=2165 $dilation=2166 $groups=3478 #2145=(1,352,4,16)f32 #weight.2=(2112,352,1,1)f32 #input.2=(1,2112,4,16)f32
nn.BatchNorm2d           effnet._blocks.22._bn0   1 1 input.2 2169 affine=True eps=1.000000e-03 num_features=2112 @bias=(2112)f32 @running_mean=(2112)f32 @running_var=(2112)f32 @weight=(2112)f32 #input.2=(1,2112,4,16)f32 #2169=(1,2112,4,16)f32
nn.SiLU                  effnet._blocks.22._swish 1 1 2169 2170 #2169=(1,2112,4,16)f32 #2170=(1,2112,4,16)f32
pnnx.Expression          pnnx_expr_254            0 1 2173 expr=None
pnnx.Expression          pnnx_expr_251            0 1 2176 expr=2112
pnnx.Attribute           effnet._blocks.22._depthwise_conv 0 1 weight.4 @data=(2112,1,3,3)f32 #weight.4=(2112,1,3,3)f32
nn.ZeroPad2d             effnet._blocks.22._depthwise_conv.static_padding 1 1 2170 2179 padding=(1,1,1,1) #2170=(1,2112,4,16)f32 #2179=(1,2112,6,18)f32
pnnx.Expression          pnnx_expr_249            0 1 2180 expr=[1,1]
pnnx.Expression          pnnx_expr_247            0 1 2181 expr=[0,0]
pnnx.Expression          pnnx_expr_244            0 1 2182 expr=[1,1]
F.conv2d                 F.conv2d_133             7 1 2179 weight.4 2173 2180 2181 2182 2176 input.4 $input=2179 $weight=weight.4 $bias=2173 $stride=2180 $padding=2181 $dilation=2182 $groups=2176 #2179=(1,2112,6,18)f32 #weight.4=(2112,1,3,3)f32 #input.4=(1,2112,4,16)f32
nn.BatchNorm2d           effnet._blocks.22._bn1   1 1 input.4 2185 affine=True eps=1.000000e-03 num_features=2112 @bias=(2112)f32 @running_mean=(2112)f32 @running_var=(2112)f32 @weight=(2112)f32 #input.4=(1,2112,4,16)f32 #2185=(1,2112,4,16)f32
nn.SiLU                  pnnx_unique_86           1 1 2185 2186 #2185=(1,2112,4,16)f32 #2186=(1,2112,4,16)f32
pnnx.Expression          pnnx_expr_242            0 1 2187 expr=[1,1]
pnnx.Attribute           effnet._blocks.22._se_reduce 0 1 bias.2 @data=(88)f32 #bias.2=(88)f32
pnnx.Attribute           pnnx_unique_87           0 1 weight.6 @data=(88,2112,1,1)f32 #weight.6=(88,2112,1,1)f32
pnnx.Expression          pnnx_expr_238            0 1 2197 expr=[1,1]
pnnx.Expression          pnnx_expr_236            0 1 2198 expr=[0,0]
pnnx.Expression          pnnx_expr_233            0 1 2199 expr=[1,1]
pnnx.Expression          pnnx_expr_232            0 1 3498 expr=1
F.adaptive_avg_pool2d    F.adaptive_avg_pool2d_22 2 1 2186 2187 x.1 $input=2186 $output_size=2187 #2186=(1,2112,4,16)f32 #x.1=(1,2112,1,1)f32
F.conv2d                 F.conv2d_134             7 1 x.1 weight.6 bias.2 2197 2198 2199 3498 input.6 $input=x.1 $weight=weight.6 $bias=bias.2 $stride=2197 $padding=2198 $dilation=2199 $groups=3498 #x.1=(1,2112,1,1)f32 #weight.6=(88,2112,1,1)f32 #bias.2=(88)f32 #input.6=(1,88,1,1)f32
nn.SiLU                  pnnx_unique_88           1 1 input.6 2202 #input.6=(1,88,1,1)f32 #2202=(1,88,1,1)f32
pnnx.Attribute           effnet._blocks.22._se_expand 0 1 bias.1 @data=(2112)f32 #bias.1=(2112)f32
pnnx.Attribute           pnnx_unique_89           0 1 weight.8 @data=(2112,88,1,1)f32 #weight.8=(2112,88,1,1)f32
pnnx.Expression          pnnx_expr_228            0 1 2211 expr=[1,1]
pnnx.Expression          pnnx_expr_226            0 1 2212 expr=[0,0]
pnnx.Expression          pnnx_expr_223            0 1 2213 expr=[1,1]
pnnx.Expression          pnnx_expr_222            0 1 3508 expr=1
F.conv2d                 F.conv2d_135             7 1 2202 weight.8 bias.1 2211 2212 2213 3508 x_squeezed.1 $input=2202 $weight=weight.8 $bias=bias.1 $stride=2211 $padding=2212 $dilation=2213 $groups=3508 #2202=(1,88,1,1)f32 #weight.8=(2112,88,1,1)f32 #bias.1=(2112)f32 #x_squeezed.1=(1,2112,1,1)f32
F.sigmoid                F.sigmoid_160            1 1 x_squeezed.1 2216 $input=x_squeezed.1 #x_squeezed.1=(1,2112,1,1)f32 #2216=(1,2112,1,1)f32
pnnx.Expression          pnnx_expr_221            2 1 2216 2186 x0.1 expr=mul(@0,@1) #2216=(1,2112,1,1)f32 #2186=(1,2112,4,16)f32 #x0.1=(1,2112,4,16)f32
pnnx.Expression          pnnx_expr_220            0 1 2220 expr=None
pnnx.Attribute           effnet._blocks.22._project_conv 0 1 weight.221 @data=(352,2112,1,1)f32 #weight.221=(352,2112,1,1)f32
pnnx.Expression          pnnx_expr_216            0 1 2226 expr=[1,1]
pnnx.Expression          pnnx_expr_214            0 1 2227 expr=[0,0]
pnnx.Expression          pnnx_expr_211            0 1 2228 expr=[1,1]
pnnx.Expression          pnnx_expr_210            0 1 3518 expr=1
F.conv2d                 F.conv2d_136             7 1 x0.1 weight.221 2220 2226 2227 2228 3518 input.177 $input=x0.1 $weight=weight.221 $bias=2220 $stride=2226 $padding=2227 $dilation=2228 $groups=3518 #x0.1=(1,2112,4,16)f32 #weight.221=(352,2112,1,1)f32 #input.177=(1,352,4,16)f32
nn.BatchNorm2d           effnet._blocks.22._bn2   1 1 input.177 2231 affine=True eps=1.000000e-03 num_features=352 @bias=(352)f32 @running_mean=(352)f32 @running_var=(352)f32 @weight=(352)f32 #input.177=(1,352,4,16)f32 #2231=(1,352,4,16)f32
pnnx.Expression          pnnx_expr_208            2 1 2231 2145 2232 expr=add(@0,@1) #2231=(1,352,4,16)f32 #2145=(1,352,4,16)f32 #2232=(1,352,4,16)f32
pnnx.Expression          pnnx_expr_207            0 1 2235 expr=None
pnnx.Attribute           effnet._conv_head        0 1 weight.1 @data=(1408,352,1,1)f32 #weight.1=(1408,352,1,1)f32
pnnx.Expression          pnnx_expr_203            0 1 2241 expr=[1,1]
pnnx.Expression          pnnx_expr_201            0 1 2242 expr=[0,0]
pnnx.Expression          pnnx_expr_198            0 1 2243 expr=[1,1]
pnnx.Expression          pnnx_expr_197            0 1 3529 expr=1
F.conv2d                 F.conv2d_137             7 1 2232 weight.1 2235 2241 2242 2243 3529 input.1 $input=2232 $weight=weight.1 $bias=2235 $stride=2241 $padding=2242 $dilation=2243 $groups=3529 #2232=(1,352,4,16)f32 #weight.1=(1408,352,1,1)f32 #input.1=(1,1408,4,16)f32
nn.BatchNorm2d           effnet._bn1              1 1 input.1 237 affine=True eps=1.000000e-03 num_features=1408 @bias=(1408)f32 @running_mean=(1408)f32 @running_var=(1408)f32 @weight=(1408)f32 #input.1=(1,1408,4,16)f32 #237=(1,1408,4,16)f32
nn.SiLU                  pnnx_unique_90           1 1 237 238 #237=(1,1408,4,16)f32 #238=(1,1408,4,16)f32
nn.AvgPool2d             avgpool                  1 1 238 241 ceil_mode=False count_include_pad=True divisor_override=None kernel_size=(4,1) padding=(0,0) stride=(4,1) #238=(1,1408,4,16)f32 #241=(1,1408,1,16)f32
pnnx.Expression          pnnx_expr_196            0 1 3533 expr=2
pnnx.Expression          pnnx_expr_195            0 1 3534 expr=3
pnnx.Expression          pnnx_expr_194            0 1 2248 expr=0
pnnx.Expression          pnnx_expr_193            0 1 2249 expr=2147483647
pnnx.Expression          pnnx_expr_192            0 1 2250 expr=1
pnnx.Expression          pnnx_expr_191            0 1 2251 expr=2
pnnx.Expression          pnnx_expr_190            0 1 2252 expr=3
pnnx.Expression          pnnx_expr_189            0 1 2253 expr=1.000000e-07
pnnx.Expression          pnnx_expr_188            0 1 2254 expr=9.999999e-01
torch.transpose          torch.transpose_228      3 1 241 3533 3534 input0.3 $input=241 $dim0=3533 $dim1=3534 #241=(1,1408,1,16)f32 #input0.3=(1,1408,16,1)f32
nn.Conv2d                attention.att.0          1 1 input0.3 2272 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2272=(1,6,16,1)f32
nn.Conv2d                attention.cla.0          1 1 input0.3 2274 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2274=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_187            0 1 3535 expr=0
F.sigmoid                F.sigmoid_161            1 1 2272 att3.1 $input=2272 #2272=(1,6,16,1)f32 #att3.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_186            0 1 3536 expr=1
pnnx.Expression          pnnx_expr_185            0 1 3537 expr=0
pnnx.Expression          pnnx_expr_184            0 1 3538 expr=2147483647
pnnx.Expression          pnnx_expr_183            0 1 3539 expr=1
pnnx.Expression          pnnx_expr_182            0 1 3540 expr=0
pnnx.Expression          pnnx_expr_181            0 1 3541 expr=2147483647
pnnx.Expression          pnnx_expr_180            0 1 3542 expr=1
pnnx.Expression          pnnx_expr_179            0 1 3543 expr=0
pnnx.Expression          pnnx_expr_178            0 1 3544 expr=0
pnnx.Expression          pnnx_expr_177            0 1 3545 expr=0
pnnx.Expression          pnnx_expr_176            0 1 3546 expr=2147483647
pnnx.Expression          pnnx_expr_175            0 1 3547 expr=1
F.sigmoid                F.sigmoid_162            1 1 2274 cla3.1 $input=2274 #2274=(1,6,16,1)f32 #cla3.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_174            0 1 3548 expr=1
pnnx.Expression          pnnx_expr_173            0 1 3549 expr=0
pnnx.Expression          pnnx_expr_172            0 1 3550 expr=2147483647
pnnx.Expression          pnnx_expr_171            0 1 3551 expr=1
pnnx.Expression          pnnx_expr_170            0 1 3552 expr=2
pnnx.Expression          pnnx_expr_169            0 1 3553 expr=0
pnnx.Expression          pnnx_expr_168            0 1 3554 expr=2147483647
pnnx.Expression          pnnx_expr_167            0 1 3555 expr=1
pnnx.Expression          pnnx_expr_166            0 1 3556 expr=3
pnnx.Expression          pnnx_expr_165            0 1 3557 expr=0
Tensor.slice             Tensor.slice_182         5 1 att3.1 2248 3535 2249 2250 2276 $input=att3.1 $dim=2248 $start=3535 $end=2249 $step=2250 #att3.1=(1,6,16,1)f32 #2276=(1,6,16,1)f32
Tensor.slice             Tensor.slice_183         5 1 2276 3536 3537 3538 3539 2277 $input=2276 $dim=3536 $start=3537 $end=3538 $step=3539 #2276=(1,6,16,1)f32 #2277=(1,6,16,1)f32
Tensor.slice             Tensor.slice_184         5 1 2277 2251 3540 3541 3542 2278 $input=2277 $dim=2251 $start=3540 $end=3541 $step=3542 #2277=(1,6,16,1)f32 #2278=(1,6,16,1)f32
Tensor.select            Tensor.select_169        3 1 2278 2252 3543 att4.1 $input=2278 $dim=2252 $index=3543 #2278=(1,6,16,1)f32 #att4.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_163            0 1 2285 expr=[2]
torch.clamp              torch.clamp_214          3 1 att4.1 2253 2254 att5.1 $input=att4.1 $min=2253 $max=2254 #att4.1=(1,6,16)f32 #att5.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_162            0 1 3559 expr=0
pnnx.Expression          pnnx_expr_161            0 1 3560 expr=0
pnnx.Expression          pnnx_expr_160            0 1 3561 expr=2147483647
pnnx.Expression          pnnx_expr_159            0 1 3562 expr=1
pnnx.Expression          pnnx_expr_158            0 1 3563 expr=1
pnnx.Expression          pnnx_expr_157            0 1 3564 expr=0
pnnx.Expression          pnnx_expr_156            0 1 3565 expr=2147483647
pnnx.Expression          pnnx_expr_155            0 1 3566 expr=1
pnnx.Expression          pnnx_expr_154            0 1 3567 expr=2
torch.sum                torch.sum_218            2 1 att5.1 2285 2286 keepdim=False $input=att5.1 $dim=2285 #att5.1=(1,6,16)f32 #2286=(1,6)f32
Tensor.slice             Tensor.slice_188         5 1 2286 3559 3560 3561 3562 2287 $input=2286 $dim=3559 $start=3560 $end=3561 $step=3562 #2286=(1,6)f32 #2287=(1,6)f32
Tensor.slice             Tensor.slice_189         5 1 2287 3563 3564 3565 3566 2288 $input=2287 $dim=3563 $start=3564 $end=3565 $step=3566 #2287=(1,6)f32 #2288=(1,6)f32
torch.unsqueeze          torch.unsqueeze_229      2 1 2288 3567 2289 $input=2288 $dim=3567 #2288=(1,6)f32 #2289=(1,6,1)f32
Tensor.slice             Tensor.slice_185         5 1 cla3.1 3544 3545 3546 3547 2280 $input=cla3.1 $dim=3544 $start=3545 $end=3546 $step=3547 #cla3.1=(1,6,16,1)f32 #2280=(1,6,16,1)f32
Tensor.slice             Tensor.slice_186         5 1 2280 3548 3549 3550 3551 2281 $input=2280 $dim=3548 $start=3549 $end=3550 $step=3551 #2280=(1,6,16,1)f32 #2281=(1,6,16,1)f32
Tensor.slice             Tensor.slice_187         5 1 2281 3552 3553 3554 3555 2282 $input=2281 $dim=3552 $start=3553 $end=3554 $step=3555 #2281=(1,6,16,1)f32 #2282=(1,6,16,1)f32
Tensor.select            Tensor.select_170        3 1 2282 3556 3557 cla4.1 $input=2282 $dim=3556 $index=3557 #2282=(1,6,16,1)f32 #cla4.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_152            3 1 att5.1 2289 cla4.1 2291 expr=mul(div(@0,@1),@2) #att5.1=(1,6,16)f32 #2289=(1,6,1)f32 #cla4.1=(1,6,16)f32 #2291=(1,6,16)f32
pnnx.Expression          pnnx_expr_150            0 1 2292 expr=[2]
torch.sum                torch.sum_219            2 1 2291 2292 2293 keepdim=False $input=2291 $dim=2292 #2291=(1,6,16)f32 #2293=(1,6)f32
pnnx.Expression          pnnx_expr_147            1 1 2293 2295 expr=mul(@0,5.921604e-01) #2293=(1,6)f32 #2295=(1,6)f32
nn.Conv2d                attention.att.1          1 1 input0.3 2296 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2296=(1,6,16,1)f32
nn.Conv2d                attention.cla.1          1 1 input0.3 2298 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2298=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_146            0 1 3573 expr=0
pnnx.Expression          pnnx_expr_145            0 1 3574 expr=0
pnnx.Expression          pnnx_expr_144            0 1 3575 expr=2147483647
pnnx.Expression          pnnx_expr_143            0 1 3576 expr=1
F.sigmoid                F.sigmoid_163            1 1 2296 att6.1 $input=2296 #2296=(1,6,16,1)f32 #att6.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_142            0 1 3577 expr=1
pnnx.Expression          pnnx_expr_141            0 1 3578 expr=0
pnnx.Expression          pnnx_expr_140            0 1 3579 expr=2147483647
pnnx.Expression          pnnx_expr_139            0 1 3580 expr=1
pnnx.Expression          pnnx_expr_138            0 1 3581 expr=2
pnnx.Expression          pnnx_expr_137            0 1 3582 expr=0
pnnx.Expression          pnnx_expr_136            0 1 3583 expr=2147483647
pnnx.Expression          pnnx_expr_135            0 1 3584 expr=1
pnnx.Expression          pnnx_expr_134            0 1 3585 expr=3
pnnx.Expression          pnnx_expr_133            0 1 3586 expr=0
pnnx.Expression          pnnx_expr_132            0 1 3587 expr=0
pnnx.Expression          pnnx_expr_131            0 1 3588 expr=0
pnnx.Expression          pnnx_expr_130            0 1 3589 expr=2147483647
pnnx.Expression          pnnx_expr_129            0 1 3590 expr=1
F.sigmoid                F.sigmoid_164            1 1 2298 cla5.1 $input=2298 #2298=(1,6,16,1)f32 #cla5.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_128            0 1 3591 expr=1
pnnx.Expression          pnnx_expr_127            0 1 3592 expr=0
pnnx.Expression          pnnx_expr_126            0 1 3593 expr=2147483647
pnnx.Expression          pnnx_expr_125            0 1 3594 expr=1
pnnx.Expression          pnnx_expr_124            0 1 3595 expr=2
pnnx.Expression          pnnx_expr_123            0 1 3596 expr=0
pnnx.Expression          pnnx_expr_122            0 1 3597 expr=2147483647
pnnx.Expression          pnnx_expr_121            0 1 3598 expr=1
pnnx.Expression          pnnx_expr_120            0 1 3599 expr=3
pnnx.Expression          pnnx_expr_119            0 1 3600 expr=0
pnnx.Expression          pnnx_expr_118            0 1 3601 expr=1.000000e-07
pnnx.Expression          pnnx_expr_117            0 1 3602 expr=9.999999e-01
Tensor.slice             Tensor.slice_190         5 1 att6.1 3573 3574 3575 3576 2300 $input=att6.1 $dim=3573 $start=3574 $end=3575 $step=3576 #att6.1=(1,6,16,1)f32 #2300=(1,6,16,1)f32
Tensor.slice             Tensor.slice_191         5 1 2300 3577 3578 3579 3580 2301 $input=2300 $dim=3577 $start=3578 $end=3579 $step=3580 #2300=(1,6,16,1)f32 #2301=(1,6,16,1)f32
Tensor.slice             Tensor.slice_192         5 1 2301 3581 3582 3583 3584 2302 $input=2301 $dim=3581 $start=3582 $end=3583 $step=3584 #2301=(1,6,16,1)f32 #2302=(1,6,16,1)f32
Tensor.select            Tensor.select_172        3 1 2302 3585 3586 att7.1 $input=2302 $dim=3585 $index=3586 #2302=(1,6,16,1)f32 #att7.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_115            0 1 2309 expr=[2]
torch.clamp              torch.clamp_215          3 1 att7.1 3601 3602 att8.1 $input=att7.1 $min=3601 $max=3602 #att7.1=(1,6,16)f32 #att8.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_114            0 1 3606 expr=0
pnnx.Expression          pnnx_expr_113            0 1 3607 expr=0
pnnx.Expression          pnnx_expr_112            0 1 3608 expr=2147483647
pnnx.Expression          pnnx_expr_111            0 1 3609 expr=1
pnnx.Expression          pnnx_expr_110            0 1 3610 expr=1
pnnx.Expression          pnnx_expr_109            0 1 3611 expr=0
pnnx.Expression          pnnx_expr_108            0 1 3612 expr=2147483647
pnnx.Expression          pnnx_expr_107            0 1 3613 expr=1
pnnx.Expression          pnnx_expr_106            0 1 3614 expr=2
torch.sum                torch.sum_220            2 1 att8.1 2309 2310 keepdim=False $input=att8.1 $dim=2309 #att8.1=(1,6,16)f32 #2310=(1,6)f32
Tensor.slice             Tensor.slice_196         5 1 2310 3606 3607 3608 3609 2311 $input=2310 $dim=3606 $start=3607 $end=3608 $step=3609 #2310=(1,6)f32 #2311=(1,6)f32
Tensor.slice             Tensor.slice_197         5 1 2311 3610 3611 3612 3613 2312 $input=2311 $dim=3610 $start=3611 $end=3612 $step=3613 #2311=(1,6)f32 #2312=(1,6)f32
torch.unsqueeze          torch.unsqueeze_230      2 1 2312 3614 2313 $input=2312 $dim=3614 #2312=(1,6)f32 #2313=(1,6,1)f32
Tensor.slice             Tensor.slice_193         5 1 cla5.1 3587 3588 3589 3590 2304 $input=cla5.1 $dim=3587 $start=3588 $end=3589 $step=3590 #cla5.1=(1,6,16,1)f32 #2304=(1,6,16,1)f32
Tensor.slice             Tensor.slice_194         5 1 2304 3591 3592 3593 3594 2305 $input=2304 $dim=3591 $start=3592 $end=3593 $step=3594 #2304=(1,6,16,1)f32 #2305=(1,6,16,1)f32
Tensor.slice             Tensor.slice_195         5 1 2305 3595 3596 3597 3598 2306 $input=2305 $dim=3595 $start=3596 $end=3597 $step=3598 #2305=(1,6,16,1)f32 #2306=(1,6,16,1)f32
Tensor.select            Tensor.select_173        3 1 2306 3599 3600 cla6.1 $input=2306 $dim=3599 $index=3600 #2306=(1,6,16,1)f32 #cla6.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_104            3 1 att8.1 2313 cla6.1 2315 expr=mul(div(@0,@1),@2) #att8.1=(1,6,16)f32 #2313=(1,6,1)f32 #cla6.1=(1,6,16)f32 #2315=(1,6,16)f32
pnnx.Expression          pnnx_expr_102            0 1 2316 expr=[2]
torch.sum                torch.sum_221            2 1 2315 2316 2317 keepdim=False $input=2315 $dim=2316 #2315=(1,6,16)f32 #2317=(1,6)f32
pnnx.Expression          pnnx_expr_99             1 1 2317 2319 expr=mul(@0,5.930342e-01) #2317=(1,6)f32 #2319=(1,6)f32
nn.Conv2d                attention.att.2          1 1 input0.3 2320 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2320=(1,6,16,1)f32
nn.Conv2d                attention.cla.2          1 1 input0.3 2322 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2322=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_98             0 1 3620 expr=0
pnnx.Expression          pnnx_expr_97             0 1 3621 expr=0
pnnx.Expression          pnnx_expr_96             0 1 3622 expr=2147483647
pnnx.Expression          pnnx_expr_95             0 1 3623 expr=1
F.sigmoid                F.sigmoid_165            1 1 2320 att9.1 $input=2320 #2320=(1,6,16,1)f32 #att9.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_94             0 1 3624 expr=1
pnnx.Expression          pnnx_expr_93             0 1 3625 expr=0
pnnx.Expression          pnnx_expr_92             0 1 3626 expr=2147483647
pnnx.Expression          pnnx_expr_91             0 1 3627 expr=1
pnnx.Expression          pnnx_expr_90             0 1 3628 expr=2
pnnx.Expression          pnnx_expr_89             0 1 3629 expr=0
pnnx.Expression          pnnx_expr_88             0 1 3630 expr=2147483647
pnnx.Expression          pnnx_expr_87             0 1 3631 expr=1
pnnx.Expression          pnnx_expr_86             0 1 3632 expr=3
pnnx.Expression          pnnx_expr_85             0 1 3633 expr=0
pnnx.Expression          pnnx_expr_84             0 1 3634 expr=0
pnnx.Expression          pnnx_expr_83             0 1 3635 expr=0
pnnx.Expression          pnnx_expr_82             0 1 3636 expr=2147483647
pnnx.Expression          pnnx_expr_81             0 1 3637 expr=1
F.sigmoid                F.sigmoid_166            1 1 2322 cla7.1 $input=2322 #2322=(1,6,16,1)f32 #cla7.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_80             0 1 3638 expr=1
pnnx.Expression          pnnx_expr_79             0 1 3639 expr=0
pnnx.Expression          pnnx_expr_78             0 1 3640 expr=2147483647
pnnx.Expression          pnnx_expr_77             0 1 3641 expr=1
pnnx.Expression          pnnx_expr_76             0 1 3642 expr=2
pnnx.Expression          pnnx_expr_75             0 1 3643 expr=0
pnnx.Expression          pnnx_expr_74             0 1 3644 expr=2147483647
pnnx.Expression          pnnx_expr_73             0 1 3645 expr=1
pnnx.Expression          pnnx_expr_72             0 1 3646 expr=3
pnnx.Expression          pnnx_expr_71             0 1 3647 expr=0
pnnx.Expression          pnnx_expr_70             0 1 3648 expr=1.000000e-07
pnnx.Expression          pnnx_expr_69             0 1 3649 expr=9.999999e-01
Tensor.slice             Tensor.slice_198         5 1 att9.1 3620 3621 3622 3623 2324 $input=att9.1 $dim=3620 $start=3621 $end=3622 $step=3623 #att9.1=(1,6,16,1)f32 #2324=(1,6,16,1)f32
Tensor.slice             Tensor.slice_199         5 1 2324 3624 3625 3626 3627 2325 $input=2324 $dim=3624 $start=3625 $end=3626 $step=3627 #2324=(1,6,16,1)f32 #2325=(1,6,16,1)f32
Tensor.slice             Tensor.slice_200         5 1 2325 3628 3629 3630 3631 2326 $input=2325 $dim=3628 $start=3629 $end=3630 $step=3631 #2325=(1,6,16,1)f32 #2326=(1,6,16,1)f32
Tensor.select            Tensor.select_175        3 1 2326 3632 3633 att10.1 $input=2326 $dim=3632 $index=3633 #2326=(1,6,16,1)f32 #att10.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_67             0 1 2333 expr=[2]
torch.clamp              torch.clamp_216          3 1 att10.1 3648 3649 att11.1 $input=att10.1 $min=3648 $max=3649 #att10.1=(1,6,16)f32 #att11.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_66             0 1 3653 expr=0
pnnx.Expression          pnnx_expr_65             0 1 3654 expr=0
pnnx.Expression          pnnx_expr_64             0 1 3655 expr=2147483647
pnnx.Expression          pnnx_expr_63             0 1 3656 expr=1
pnnx.Expression          pnnx_expr_62             0 1 3657 expr=1
pnnx.Expression          pnnx_expr_61             0 1 3658 expr=0
pnnx.Expression          pnnx_expr_60             0 1 3659 expr=2147483647
pnnx.Expression          pnnx_expr_59             0 1 3660 expr=1
pnnx.Expression          pnnx_expr_58             0 1 3661 expr=2
torch.sum                torch.sum_222            2 1 att11.1 2333 2334 keepdim=False $input=att11.1 $dim=2333 #att11.1=(1,6,16)f32 #2334=(1,6)f32
Tensor.slice             Tensor.slice_204         5 1 2334 3653 3654 3655 3656 2335 $input=2334 $dim=3653 $start=3654 $end=3655 $step=3656 #2334=(1,6)f32 #2335=(1,6)f32
Tensor.slice             Tensor.slice_205         5 1 2335 3657 3658 3659 3660 2336 $input=2335 $dim=3657 $start=3658 $end=3659 $step=3660 #2335=(1,6)f32 #2336=(1,6)f32
torch.unsqueeze          torch.unsqueeze_231      2 1 2336 3661 2337 $input=2336 $dim=3661 #2336=(1,6)f32 #2337=(1,6,1)f32
Tensor.slice             Tensor.slice_201         5 1 cla7.1 3634 3635 3636 3637 2328 $input=cla7.1 $dim=3634 $start=3635 $end=3636 $step=3637 #cla7.1=(1,6,16,1)f32 #2328=(1,6,16,1)f32
Tensor.slice             Tensor.slice_202         5 1 2328 3638 3639 3640 3641 2329 $input=2328 $dim=3638 $start=3639 $end=3640 $step=3641 #2328=(1,6,16,1)f32 #2329=(1,6,16,1)f32
Tensor.slice             Tensor.slice_203         5 1 2329 3642 3643 3644 3645 2330 $input=2329 $dim=3642 $start=3643 $end=3644 $step=3645 #2329=(1,6,16,1)f32 #2330=(1,6,16,1)f32
Tensor.select            Tensor.select_176        3 1 2330 3646 3647 cla8.1 $input=2330 $dim=3646 $index=3647 #2330=(1,6,16,1)f32 #cla8.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_56             3 1 att11.1 2337 cla8.1 2339 expr=mul(div(@0,@1),@2) #att11.1=(1,6,16)f32 #2337=(1,6,1)f32 #cla8.1=(1,6,16)f32 #2339=(1,6,16)f32
pnnx.Expression          pnnx_expr_54             0 1 2340 expr=[2]
torch.sum                torch.sum_223            2 1 2339 2340 2341 keepdim=False $input=2339 $dim=2340 #2339=(1,6,16)f32 #2341=(1,6)f32
pnnx.Expression          pnnx_expr_51             1 1 2341 2343 expr=mul(@0,6.005406e-01) #2341=(1,6)f32 #2343=(1,6)f32
nn.Conv2d                attention.att.3          1 1 input0.3 2344 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2344=(1,6,16,1)f32
nn.Conv2d                attention.cla.3          1 1 input0.3 2346 bias=True dilation=(1,1) groups=1 in_channels=1408 kernel_size=(1,1) out_channels=6 padding=(0,0) padding_mode=zeros stride=(1,1) @bias=(6)f32 @weight=(6,1408,1,1)f32 #input0.3=(1,1408,16,1)f32 #2346=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_50             0 1 3667 expr=0
pnnx.Expression          pnnx_expr_49             0 1 3668 expr=0
pnnx.Expression          pnnx_expr_48             0 1 3669 expr=2147483647
pnnx.Expression          pnnx_expr_47             0 1 3670 expr=1
F.sigmoid                F.sigmoid_167            1 1 2344 att12.1 $input=2344 #2344=(1,6,16,1)f32 #att12.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_46             0 1 3671 expr=1
pnnx.Expression          pnnx_expr_45             0 1 3672 expr=0
pnnx.Expression          pnnx_expr_44             0 1 3673 expr=2147483647
pnnx.Expression          pnnx_expr_43             0 1 3674 expr=1
pnnx.Expression          pnnx_expr_42             0 1 3675 expr=2
pnnx.Expression          pnnx_expr_41             0 1 3676 expr=0
pnnx.Expression          pnnx_expr_40             0 1 3677 expr=2147483647
pnnx.Expression          pnnx_expr_39             0 1 3678 expr=1
pnnx.Expression          pnnx_expr_38             0 1 3679 expr=3
pnnx.Expression          pnnx_expr_37             0 1 3680 expr=0
pnnx.Expression          pnnx_expr_36             0 1 3681 expr=0
pnnx.Expression          pnnx_expr_35             0 1 3682 expr=0
pnnx.Expression          pnnx_expr_34             0 1 3683 expr=2147483647
pnnx.Expression          pnnx_expr_33             0 1 3684 expr=1
F.sigmoid                F.sigmoid_168            1 1 2346 cla9.1 $input=2346 #2346=(1,6,16,1)f32 #cla9.1=(1,6,16,1)f32
pnnx.Expression          pnnx_expr_32             0 1 3685 expr=1
pnnx.Expression          pnnx_expr_31             0 1 3686 expr=0
pnnx.Expression          pnnx_expr_30             0 1 3687 expr=2147483647
pnnx.Expression          pnnx_expr_29             0 1 3688 expr=1
pnnx.Expression          pnnx_expr_28             0 1 3689 expr=2
pnnx.Expression          pnnx_expr_27             0 1 3690 expr=0
pnnx.Expression          pnnx_expr_26             0 1 3691 expr=2147483647
pnnx.Expression          pnnx_expr_25             0 1 3692 expr=1
pnnx.Expression          pnnx_expr_24             0 1 3693 expr=3
pnnx.Expression          pnnx_expr_23             0 1 3694 expr=0
pnnx.Expression          pnnx_expr_22             0 1 3695 expr=1.000000e-07
pnnx.Expression          pnnx_expr_21             0 1 3696 expr=9.999999e-01
Tensor.slice             Tensor.slice_206         5 1 att12.1 3667 3668 3669 3670 2348 $input=att12.1 $dim=3667 $start=3668 $end=3669 $step=3670 #att12.1=(1,6,16,1)f32 #2348=(1,6,16,1)f32
Tensor.slice             Tensor.slice_207         5 1 2348 3671 3672 3673 3674 2349 $input=2348 $dim=3671 $start=3672 $end=3673 $step=3674 #2348=(1,6,16,1)f32 #2349=(1,6,16,1)f32
Tensor.slice             Tensor.slice_208         5 1 2349 3675 3676 3677 3678 2350 $input=2349 $dim=3675 $start=3676 $end=3677 $step=3678 #2349=(1,6,16,1)f32 #2350=(1,6,16,1)f32
Tensor.select            Tensor.select_178        3 1 2350 3679 3680 att13.1 $input=2350 $dim=3679 $index=3680 #2350=(1,6,16,1)f32 #att13.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_19             0 1 2357 expr=[2]
torch.clamp              torch.clamp_217          3 1 att13.1 3695 3696 att14.1 $input=att13.1 $min=3695 $max=3696 #att13.1=(1,6,16)f32 #att14.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_18             0 1 3700 expr=0
pnnx.Expression          pnnx_expr_17             0 1 3701 expr=0
pnnx.Expression          pnnx_expr_16             0 1 3702 expr=2147483647
pnnx.Expression          pnnx_expr_15             0 1 3703 expr=1
pnnx.Expression          pnnx_expr_14             0 1 3704 expr=1
pnnx.Expression          pnnx_expr_13             0 1 3705 expr=0
pnnx.Expression          pnnx_expr_12             0 1 3706 expr=2147483647
pnnx.Expression          pnnx_expr_11             0 1 3707 expr=1
pnnx.Expression          pnnx_expr_10             0 1 3708 expr=2
torch.sum                torch.sum_224            2 1 att14.1 2357 2358 keepdim=False $input=att14.1 $dim=2357 #att14.1=(1,6,16)f32 #2358=(1,6)f32
Tensor.slice             Tensor.slice_212         5 1 2358 3700 3701 3702 3703 2359 $input=2358 $dim=3700 $start=3701 $end=3702 $step=3703 #2358=(1,6)f32 #2359=(1,6)f32
Tensor.slice             Tensor.slice_213         5 1 2359 3704 3705 3706 3707 2360 $input=2359 $dim=3704 $start=3705 $end=3706 $step=3707 #2359=(1,6)f32 #2360=(1,6)f32
torch.unsqueeze          torch.unsqueeze_232      2 1 2360 3708 2361 $input=2360 $dim=3708 #2360=(1,6)f32 #2361=(1,6,1)f32
Tensor.slice             Tensor.slice_209         5 1 cla9.1 3681 3682 3683 3684 2352 $input=cla9.1 $dim=3681 $start=3682 $end=3683 $step=3684 #cla9.1=(1,6,16,1)f32 #2352=(1,6,16,1)f32
Tensor.slice             Tensor.slice_210         5 1 2352 3685 3686 3687 3688 2353 $input=2352 $dim=3685 $start=3686 $end=3687 $step=3688 #2352=(1,6,16,1)f32 #2353=(1,6,16,1)f32
Tensor.slice             Tensor.slice_211         5 1 2353 3689 3690 3691 3692 2354 $input=2353 $dim=3689 $start=3690 $end=3691 $step=3692 #2353=(1,6,16,1)f32 #2354=(1,6,16,1)f32
Tensor.select            Tensor.select_179        3 1 2354 3693 3694 cla10.1 $input=2354 $dim=3693 $index=3694 #2354=(1,6,16,1)f32 #cla10.1=(1,6,16)f32
pnnx.Expression          pnnx_expr_8              3 1 att14.1 2361 cla10.1 2363 expr=mul(div(@0,@1),@2) #att14.1=(1,6,16)f32 #2361=(1,6,1)f32 #cla10.1=(1,6,16)f32 #2363=(1,6,16)f32
pnnx.Expression          pnnx_expr_6              0 1 2364 expr=[2]
torch.sum                torch.sum_225            2 1 2363 2364 2365 keepdim=False $input=2363 $dim=2364 #2363=(1,6,16)f32 #2365=(1,6)f32
pnnx.Expression          pnnx_expr_3              1 1 2365 2367 expr=mul(@0,5.926947e-01) #2365=(1,6)f32 #2367=(1,6)f32
pnnx.Expression          pnnx_expr_2              0 1 3714 expr=0
pnnx.Expression          pnnx_expr_0              0 1 2370 expr=[0]
torch.stack              torch.stack_181          5 1 2295 2319 2343 2367 3714 2369 $dim=3714 #2295=(1,6)f32 #2319=(1,6)f32 #2343=(1,6)f32 #2367=(1,6)f32 #2369=(4,1,6)f32
torch.sum                torch.sum_226            2 1 2369 2370 2371 keepdim=False $input=2369 $dim=2370 #2369=(4,1,6)f32 #2371=(1,6)f32
pnnx.Output              pnnx_output_0            1 0 2371 #2371=(1,6)f32
